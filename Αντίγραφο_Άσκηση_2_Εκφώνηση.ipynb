{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Αντίγραφο Άσκηση 2 Εκφώνηση.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.5",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLFXD3IePSyL"
      },
      "source": [
        "# Εργαστηριακή Άσκηση 2. Μη επιβλεπόμενη μάθηση. \n",
        "## Σύστημα συστάσεων βασισμένο στο περιεχόμενο\n",
        "## Σημασιολογική απεικόνιση δεδομένων με χρήση SOM \n",
        "Ημερομηνία εκφώνησης της άσκησης: 23 Νοεμβρίου 2020\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5wbBzIYnird",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bfd4538-14d5-4000-abe6-9f5c983f3f45"
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade pandas\n",
        "!pip install --upgrade nltk\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install --upgrade joblib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/eb/4a3642e971f404d69d4f6fa3885559d67562801b99d7592487f1ecc4e017/pip-20.3.3-py2.py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 5.2MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-20.3.3\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.19.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.19.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.5.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (1.0.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.41.1)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434676 sha256=8140ff5eb42fc2cf98b6502a1bc035f381cd94f772baad44f9f43e94029a1bd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/5e/42/64abaeca668161c3e2cecc24f864a8fc421e3d07a104fc8a51\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.5\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-0.24.0-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.2 MB 86.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.19.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.0.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.0 threadpoolctl-2.1.0\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aViHqlQcPSyP"
      },
      "source": [
        "## Εισαγωγή του Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZVmdDExPSyQ"
      },
      "source": [
        "Το σύνολο δεδομένων με το οποίο θα δουλέψουμε είναι βασισμένο στο [Carnegie Mellon Movie Summary Corpus](http://www.cs.cmu.edu/~ark/personas/). Πρόκειται για ένα dataset με περίπου 40.000 περιγραφές ταινιών. Η περιγραφή κάθε ταινίας αποτελείται από τον τίτλο της, μια ή περισσότερες ετικέτες που χαρακτηρίζουν το είδος της ταινίας και τέλος τη σύνοψη της υπόθεσής της. Αρχικά εισάγουμε το dataset (χρησιμοποιήστε αυτούσιο τον κώδικα, δεν χρειάζεστε το αρχείο csv) στο dataframe `df_data_1`: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62SOj46gPSyS"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset_url = \"https://drive.google.com/uc?export=download&id=1PdkVDENX12tQliCk_HtUnAUbfxXvnWuG\"\n",
        "df_data_1 = pd.read_csv(dataset_url, sep='\\t',  header=None, quoting=3, error_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TAEZGdIPSyW"
      },
      "source": [
        "Κάθε ομάδα θα δουλέψει σε ένα μοναδικό υποσύνολο 5.000 ταινιών (διαφορετικό dataset για κάθε ομάδα) ως εξής\n",
        "\n",
        "1. Κάθε ομάδα έχει έναν αριθμό \"seed\" (φύτρο) που είναι ο ίδιος με τον αριθμό της ομάδας σας: θα τον βρείτε στην κολόνα Α/Α [εδώ](https://docs.google.com/spreadsheets/d/1CD6AtX7YnocXceCELl_XJ06kyRr0YQPhor8dpw012t0/edit?usp=sharing).\n",
        "\n",
        "2. Το data frame `df_data_2` έχει γραμμές όσες και οι ομάδες και 5.000 στήλες. Σε κάθε ομάδα αντιστοιχεί η γραμμή του πίνακα με το `team_seed_number` της. Η γραμμή αυτή θα περιλαμβάνει 5.000 διαφορετικούς αριθμούς που αντιστοιχούν σε ταινίες του αρχικού dataset. \n",
        "\n",
        "3. Στο επόμενο κελί αλλάξτε τη μεταβλητή `team_seed_number` με το Seed της ομάδας σας.\n",
        "\n",
        "4. Τρέξτε τον κώδικα. Θα προκύψουν τα μοναδικά για κάθε ομάδα  titles, categories, catbins, summaries και corpus με τα οποία θα δουλέψετε."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2POlqDjkPSyY"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# βάλτε το seed που αντιστοιχεί στην ομάδα σας\n",
        "team_seed_number = 15\n",
        "\n",
        "movie_seeds_url = \"https://drive.google.com/uc?export=download&id=1EA_pUIgK5Ub3kEzFbFl8wSRqAV6feHqD\"\n",
        "df_data_2 = pd.read_csv(movie_seeds_url, header=None, error_bad_lines=False)\n",
        "\n",
        "# επιλέγεται \n",
        "my_index = df_data_2.iloc[team_seed_number,:].values\n",
        "\n",
        "titles = df_data_1.iloc[:, [2]].values[my_index] # movie titles (string)\n",
        "categories = df_data_1.iloc[:, [3]].values[my_index] # movie categories (string)\n",
        "bins = df_data_1.iloc[:, [4]]\n",
        "catbins = bins[4].str.split(',', expand=True).values.astype(np.float)[my_index] # movie categories in binary form (1 feature per category)\n",
        "summaries =  df_data_1.iloc[:, [5]].values[my_index] # movie summaries (string)\n",
        "corpus = summaries[:,0].tolist() # list form of summaries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If66lkwxPSyb"
      },
      "source": [
        "- Ο πίνακας **titles** περιέχει τους τίτλους των ταινιών. Παράδειγμα: 'Sid and Nancy'.\n",
        "- O πίνακας **categories** περιέχει τις κατηγορίες (είδη) της ταινίας υπό τη μορφή string. Παράδειγμα: '\"Tragedy\",  \"Indie\",  \"Punk rock\",  \"Addiction Drama\",  \"Cult\",  \"Musical\",  \"Drama\",  \"Biopic \\[feature\\]\",  \"Romantic drama\",  \"Romance Film\",  \"Biographical film\"'. Παρατηρούμε ότι είναι μια comma separated λίστα strings, με κάθε string να είναι μια κατηγορία.\n",
        "- Ο πίνακας **catbins** περιλαμβάνει πάλι τις κατηγορίες των ταινιών αλλά σε δυαδική μορφή ([one hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)). Έχει διαστάσεις 5.000 x 322 (όσες οι διαφορετικές κατηγορίες). Αν η ταινία ανήκει στο συγκεκριμένο είδος η αντίστοιχη στήλη παίρνει την τιμή 1, αλλιώς παίρνει την τιμή 0.\n",
        "- Ο πίνακας **summaries** και η λίστα **corpus** περιλαμβάνουν τις συνόψεις των ταινιών (η corpus είναι απλά ο summaries σε μορφή λίστας). Κάθε σύνοψη είναι ένα (συνήθως μεγάλο) string. Παράδειγμα: *'The film is based on the real story of a Soviet Internal Troops soldier who killed his entire unit  as a result of Dedovschina. The plot unfolds mostly on board of the prisoner transport rail car guarded by a unit of paramilitary conscripts.'*\n",
        "- Θεωρούμε ως **ID** της κάθε ταινίας τον αριθμό γραμμής της ή το αντίστοιχο στοιχείο της λίστας. Παράδειγμα: για να τυπώσουμε τη σύνοψη της ταινίας με `ID=99` (την εκατοστή) θα γράψουμε `print(corpus[99])`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_7A3KXLp0qS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef010e9-c854-43a9-d0df-084e9e3a3a98"
      },
      "source": [
        "ID = 99\n",
        "print(titles[ID])\n",
        "print(categories[ID])\n",
        "print(catbins[ID])\n",
        "print(corpus[ID])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Congkak']\n",
            "['\"Horror\"']\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Kazman is a dedicated husband who, despite his wife Sufiah's protests, decided to buy a bungalow for their family near a lake as a family getaway. Anxious in the new house, Sufiah feels as if someone is watching her. Her daughter Lisa, who would always go downstairs at night to play congkak with someone whom she could only see, compounds her uneasiness. Sufiah throws the congkak in the lake, but is awakened the next night by the sound of the congkak being played again and upon investigation, she sees an old lady playing it. When her daughter disappears, the oldest resident in that area, Pak Tua, comes to the family's rescue and helps in locating the missing family members.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTNgwBfjPSyc"
      },
      "source": [
        "# Εφαρμογή 1. Υλοποίηση συστήματος συστάσεων ταινιών βασισμένο στο περιεχόμενο\n",
        "<img src=\"http://clture.org/wp-content/uploads/2015/12/Netflix-Streaming-End-of-Year-Posts.jpg\" width=\"70%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnA2RP8GPSyf"
      },
      "source": [
        "Η πρώτη εφαρμογή που θα αναπτύξετε θα είναι ένα [σύστημα συστάσεων](https://en.wikipedia.org/wiki/Recommender_system) ταινιών βασισμένο στο περιεχόμενο (content based recommender system). Τα συστήματα συστάσεων στοχεύουν στο να προτείνουν αυτόματα στο χρήστη αντικείμενα από μια συλλογή τα οποία ιδανικά θέλουμε να βρει ενδιαφέροντα ο χρήστης. Η κατηγοριοποίηση των συστημάτων συστάσεων βασίζεται στο πώς γίνεται η επιλογή (filtering) των συστηνόμενων αντικειμένων. Οι δύο κύριες κατηγορίες είναι η συνεργατική διήθηση (collaborative filtering) όπου το σύστημα προτείνει στο χρήστη αντικείμενα που έχουν αξιολογηθεί θετικά από χρήστες που έχουν παρόμοιο με αυτόν ιστορικό αξιολογήσεων και η διήθηση με βάση το περιεχόμενο (content based filtering), όπου προτείνονται στο χρήστη αντικείμενα με παρόμοιο περιεχόμενο (με βάση κάποια χαρακτηριστικά) με αυτά που έχει προηγουμένως αξιολογήσει θετικά.\n",
        "\n",
        "Το σύστημα συστάσεων που θα αναπτύξετε θα βασίζεται στο **περιεχόμενο** και συγκεκριμένα στις συνόψεις των ταινιών (corpus). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD5KuSKrxQ8I"
      },
      "source": [
        "## Μετατροπή σε TFIDF\n",
        "\n",
        "Το πρώτο βήμα θα είναι λοιπόν να μετατρέψετε το corpus σε αναπαράσταση tf-idf:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5YP6XCZPSyh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22a98b04-2294-4880-a087-0a86254be502"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "def preprocess(s) : \n",
        "  stemmer = nltk.stem.PorterStemmer()\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  s = (s.lower()).translate(table)\n",
        "  s = re.sub(r'\\d+', 'num', s)\n",
        "  words = s.split()\n",
        "  return \" \".join([stemmer.stem(word) for word in words])\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(min_df = 0.01, stop_words='english', preprocessor=preprocess)\n",
        "vectorizer.fit(corpus)\n",
        "corpus_tf_idf = vectorizer.transform(corpus)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7pzfu6tXOXV"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "\r\n",
        "vec = CountVectorizer().fit(corpus)\r\n",
        "bag_of_words = vec.transform(corpus)\r\n",
        "sum_words = bag_of_words.sum(axis=0) \r\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\r\n",
        "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABpmcj5MXdVc",
        "outputId": "7e9de599-39e6-445a-fe13-c1288e6b7936"
      },
      "source": [
        "words_freq[120:150]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('again', 1154),\n",
              " ('if', 1149),\n",
              " ('becomes', 1134),\n",
              " ('leaves', 1130),\n",
              " ('soon', 1116),\n",
              " ('friends', 1108),\n",
              " ('comes', 1100),\n",
              " ('years', 1099),\n",
              " ('men', 1086),\n",
              " ('does', 1080),\n",
              " ('leave', 1056),\n",
              " ('girl', 1046),\n",
              " ('school', 1032),\n",
              " ('see', 1023),\n",
              " ('named', 1014),\n",
              " ('finally', 1013),\n",
              " ('asks', 1000),\n",
              " ('three', 998),\n",
              " ('meanwhile', 995),\n",
              " ('kill', 987),\n",
              " ('end', 984),\n",
              " ('brother', 982),\n",
              " ('eventually', 966),\n",
              " ('room', 961),\n",
              " ('people', 961),\n",
              " ('having', 952),\n",
              " ('each', 952),\n",
              " ('group', 952),\n",
              " ('returns', 949),\n",
              " ('town', 946)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1iepgEXxzDI"
      },
      "source": [
        "vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-uRZK3EPSyl"
      },
      "source": [
        "Η συνάρτηση [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) όπως καλείται εδώ **δεν είναι βελτιστοποιημένη**. Οι επιλογές των μεθόδων και παραμέτρων της μπορεί να έχουν **δραματική επίδραση στην ποιότητα των συστάσεων** και είναι διαφορετικές για κάθε dataset. Επίσης, οι επιλογές αυτές έχουν πολύ μεγάλη επίδραση και στη **διαστατικότητα και όγκο των δεδομένων**. Η διαστατικότητα των δεδομένων με τη σειρά της θα έχει πολύ μεγάλη επίδραση στους **χρόνους εκπαίδευσης**, ιδιαίτερα στη δεύτερη εφαρμογή της άσκησης. Ανατρέξτε στα notebooks του εργαστηρίου και στο [FAQ](https://docs.google.com/document/d/1hou1gWXQuHAB7J2aV44xm_CtAWJ63q6Cu1V6OwyL_n0/edit?usp=sharing) των ασκήσεων.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_Cw0brpnisF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1809fd57-7829-47ad-f0eb-c1ad14c9babe"
      },
      "source": [
        "print(corpus_tf_idf.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 1928)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LsmvSyVykTU"
      },
      "source": [
        "## Υλοποίηση του συστήματος συστάσεων\n",
        "\n",
        "Το σύστημα συστάσεων που θα παραδώσετε θα είναι μια συνάρτηση `content_recommender` με δύο ορίσματα `target_movie` και `max_recommendations`. Στην `target_movie` περνάμε το ID μιας ταινίας-στόχου για την οποία μας ενδιαφέρει να βρούμε παρόμοιες ως προς το περιεχόμενο (τη σύνοψη) ταινίες, `max_recommendations` στο πλήθος.\n",
        "Υλοποιήστε τη συνάρτηση ως εξής: \n",
        "- για την ταινία-στόχο, από το `corpus_tf_idf` υπολογίστε την [ομοιότητα συνημιτόνου](https://en.wikipedia.org/wiki/Cosine_similarity) της με όλες τις ταινίες της συλλογής σας\n",
        "- με βάση την ομοιότητα συνημιτόνου που υπολογίσατε, δημιουργήστε ταξινομημένο πίνακα από το μεγαλύτερο στο μικρότερο, με τα indices (`ID`) των ταινιών. Παράδειγμα: αν η ταινία με index 1 έχει ομοιότητα συνημιτόνου με 3 ταινίες \\[0.2 1 0.6\\] (έχει ομοιότητα 1 με τον εαύτό της) ο ταξινομημένος αυτός πίνακας indices θα είναι \\[1 2 0\\].\n",
        "- Για την ταινία-στόχο εκτυπώστε: id, τίτλο, σύνοψη, κατηγορίες (categories)\n",
        "- Για τις `max_recommendations` ταινίες (πλην της ίδιας της ταινίας-στόχου που έχει cosine similarity 1 με τον εαυτό της) με τη μεγαλύτερη ομοιότητα συνημιτόνου (σε φθίνουσα σειρά), τυπώστε σειρά σύστασης (1 πιο κοντινή, 2 η δεύτερη πιο κοντινή κλπ), id, τίτλο, σύνοψη, κατηγορίες (categories)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVeo6iscH6mU"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "\r\n",
        "def content_recommender(target_movie, max_recommendations):\r\n",
        "  target_corpus = corpus_tf_idf[target_movie]\r\n",
        "  N = corpus_tf_idf.shape[0]\r\n",
        "  cos_sims = np.zeros(N)\r\n",
        "  for i in range(N):\r\n",
        "    cos_sims[i] = cosine_similarity(target_corpus, corpus_tf_idf[i])\r\n",
        "  cos_sims_ind = np.argsort(cos_sims)[::-1]\r\n",
        "  print(\"Target movie:\\nId: {0}, Title: {1}, Summary: {2}, Categories: {3}\".format(\r\n",
        "        target_movie, titles[target_movie][0], summaries[target_movie][0], categories[target_movie][0]))\r\n",
        "  print(\"Recommendations:\")\r\n",
        "  i = 1\r\n",
        "  for x in cos_sims_ind[1:max_recommendations]:\r\n",
        "    print(\"{0}, Id: {1}, Title: {2}, Summary: {3}, Categories: {4}\".format(\r\n",
        "          i, x, titles[x][0], summaries[x][0], categories[x][0]))\r\n",
        "    i += 1\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRBC-tNcMh6k",
        "outputId": "f11dbfbd-47ba-4d1e-a822-5e94ae2ad133"
      },
      "source": [
        "content_recommender(99, 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target movie:\n",
            "Id: 99, Title: Congkak, Summary: Kazman is a dedicated husband who, despite his wife Sufiah's protests, decided to buy a bungalow for their family near a lake as a family getaway. Anxious in the new house, Sufiah feels as if someone is watching her. Her daughter Lisa, who would always go downstairs at night to play congkak with someone whom she could only see, compounds her uneasiness. Sufiah throws the congkak in the lake, but is awakened the next night by the sound of the congkak being played again and upon investigation, she sees an old lady playing it. When her daughter disappears, the oldest resident in that area, Pak Tua, comes to the family's rescue and helps in locating the missing family members., Categories: \"Horror\"\n",
            "Recommendations:\n",
            "1, Id: 3241, Title: The Raccoons on Ice, Summary: It is winter once again in the Evergreen Forest, home of the frozen Evergreen lake, where everyone in the forest goes to ice skate and play hockey. Schaeffer, the old sheepdog, goes down to the lake, where he and his friends The Raccoons  go to play a round of hockey. Cedrc Sneer is also at the rink, too, where he meets Sophia Tutu, a female aardvark figure skater who takes an interest in Cedric. Meanwhile, Cyril Sneer has plans for Evergreen Lake - he plans on building his 'Cyril Dome' over the entire lake. But as Cyril and his construction crew are ready to build, the Raccoons and their friends stop them in their tracks and try to defend their lake. Bert then steps in and proposes that the two warring sides should play a hockey game to determine who gets the lake. Before Bert's friends try to stop him, Bert and Cyril seal the deal. While Cyril trains his Bears for the game, the Raccoons ask Cedric to play on their side. Cedric agrees, as long as he can get away from his father, who disapproves of Cedric's friendship with the Raccoons. As they train, Cedric is shown to be a great hockey player and the team feel like they have a good chance against Cyril's team, but Cyril goes to spy on the Raccoons and he is immediately not happy with his son siding with the Raccoons. He grounds Cedric for a month and he also forbids Cedric from seeing Sophia  ever again. Despite the setback, the Raccoons and Schaeffer continue to practice. On the night before the game, the friends decide to sneak into Cyril Sneer's mansion to ask Cedric if he will play alongside them. However, Cedric is more concerned for their safety and warns them of what could happen if Cyril finds them. A moment later, they are almost caught by Cyril, who claims he can hear other voices and angrily threatens his son not to go anywhere near the Raccoons again. Because of this, Cedric becomes too scared and the friends leave without his backing. On the night of the game, Cyril's Bears are pummelling the Raccoons team by 3-0. After Bert injures his hand, Sophia finally goes up to Cedric, who has been dragged along to watch the game, and convinces him to help out. Masquerading as a \"mystery player\", Cedric manages to tie the game 3-3. Cyril finds out who the mystery player is, literally kicking his own son off the ice and threatening to lock him in the dungeon as a punishment. The team considers giving up, but despite his injury, Bert refuses to give up so easily. The rest of the team are spurred on by Bert's courage and they go back onto the ice to continue. Bert manages to win the game for the team in the last few seconds, thus saving Evergreen Lake from Cyril's greed. The next day, Julie, Tommy and their father arrive at the lake and find skate marks from the game. They're utterly puzzled as to who left them, as the humans were never aware of the threat posed to the lake, leaving Dan to curiously ask the kids if they've \"been playing any wild hockey games\" lately., Categories: \"Short Film\"\n",
            "2, Id: 1620, Title: On the Way Home, Summary: A family loses their daughter, Sarah, in an car accident. A friend of the family, a Latter-day Saint, explains that they can be reunited with their daughter after their lives by living the gospel, and two sister missionaries begin to teach the family. The family is baptised, again filled with hope for the future., Categories: \"Short Film\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "HEy8g3dNVGLf",
        "outputId": "9e2e0849-c3b9-42b0-ebd6-5a6b20286a90"
      },
      "source": [
        "corpus[1620]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A family loses their daughter, Sarah, in an car accident. A friend of the family, a Latter-day Saint, explains that they can be reunited with their daughter after their lives by living the gospel, and two sister missionaries begin to teach the family. The family is baptised, again filled with hope for the future.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QshfnfWttmEe",
        "outputId": "2938b1ff-b07e-4056-e1d1-c82440a65066"
      },
      "source": [
        "ID = 42\r\n",
        "print(titles[ID])\r\n",
        "print(categories[ID])\r\n",
        "print(catbins[ID])\r\n",
        "print(corpus[ID])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Bombay Calling']\n",
            "['\"Culture & Society\",  \"Documentary\"']\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Bombay Calling delves into the lives of a group of young Indians working outsourced jobs at a call center in Bombay. Without focusing too much on the politics, it profiles several characters as they train for and attempt to sell phone services to clients in the UK. The film shows both sides of the impact of globalization on India - the economic benefits, but also the break with tradition and loss of innocence the characters face. By the end of the film, the telemarketing venture has failed but the characters are resilient. For this reason, the film has been compared to Startup.com.http://www.eye.net/eye/issue/issue_08.10.06/film/onscreen_2.php\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKaEkVNat0OR",
        "outputId": "1b474380-60fe-4638-dbbc-b513f134fd32"
      },
      "source": [
        "content_recommender(ID, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target movie:\n",
            "Id: 42, Title: Bombay Calling, Summary: Bombay Calling delves into the lives of a group of young Indians working outsourced jobs at a call center in Bombay. Without focusing too much on the politics, it profiles several characters as they train for and attempt to sell phone services to clients in the UK. The film shows both sides of the impact of globalization on India - the economic benefits, but also the break with tradition and loss of innocence the characters face. By the end of the film, the telemarketing venture has failed but the characters are resilient. For this reason, the film has been compared to Startup.com.http://www.eye.net/eye/issue/issue_08.10.06/film/onscreen_2.php, Categories: \"Culture & Society\",  \"Documentary\"\n",
            "Recommendations:\n",
            "1, Id: 4495, Title: Tantric Tourists, Summary: A group of American Tourists go to India in search of enlightenment. The main character in the film is group leader Laurie Handlers, a larger-than-life guru from New York City who practices and teaches tantra and leads a small group of travelers on their first trip to India. The film is a classic mythical journey with the characters learning from their travels. Although superficially the film may appear to be a mockumentary, all participants are real people and nothing was staged in the filming. As such the film can be termed a fly-on-the-wall film and has been compared to This Is Spinal Tap for its outrageous and hilarious characters., Categories: \"Documentary\"\n",
            "2, Id: 416, Title: Hawayein, Summary: \"Hawayein-Winds Change But The Scars Of History Remain\" is a film which emerges from the consequences of the Operation Blue Star & is based on the aftermath of Indira Gandhi’s assassination and the nationwide 1984 Anti-Sikh riots & the subsequent victimization of the people in Punjab  in the years that followed. This film depicts real life events & most of the situations shown in this film are authentic seen through the eyes of the central protagonist ‘Sarabjit’. It is the story of his journey from innocence to disillusionment, from being a simple, romantic, music loving student, to becoming one of the \"most wanted terrorists\" in the country. The Indian Government called them terrorists but many call them martyrs. This film is an honest exploration of the reasons which led to the angst of the youth of Punjab & the turmoils suffered by their families. It is the story of a time :- #When innocents were forced to rebel against injustice & labeled terrorists by the existing government. #When political motivations created terrorists out of common criminals. #When the nations watchdogs became greedy, heartless money-making murdererous machines. #When the sound of winds created unspeakable fear in the minds of people in Punjab. #A time created by an impotent administration & judiciary & the silence of the intelligentsia. #A time which saw the beginning of cross border terrorism in India. #It exposes the real reason of Pakistan participating in & actively aiding terrorism in Punjab. This film is the first of its kind subjects to be made and has been crafted within commercial parameters. The production values of this film are on par with most major commercial Bollywood films. It has been shot in more than 41 authentic & picturesque locations in Punjab, Himachal Pradesh, Delhi & on massive sets in Kamalistan Studio- Mumbai. No effort has been spared in the making of this film. This film also shattered previous stereotypes of Sikh characters shown on the Indian screen. It has been shot on 35mm cinemascope using the latest camera Arri 435 & was released with Dolby Digital sound. It was slated for an April release & has become a much hyped media event and a memorable celluloid explosion on the Hindi screen. Beautiful locales, memorable performances, authentic costumes are the main assets of this ambitious project. This film is a brave attempt to present history in a realistic manner on a colorful tapestry woven with all human emotions of romance, tragedy, humour all presented together on a platter of wonderful music., Categories: \"Historical Epic\",  \"Drama\"\n",
            "3, Id: 1241, Title: Main Anna Nahin Hoon, Summary: Main Anna Nahin Hoon explores the dilemma of a common man in India, wherein he often finds himself caught between corruption and honesty. There is a character in the film whose personality has been modeled on social activist Anna Hazare. The film's message is that while it has become fashionable in India to support Anna Hazare's stand against corruption, how many of us are actually honest? The film takes a deeper look into the practical impossibility of living an honest life in India. The film has been written and directed by writer-director Manish Gupta known for writing 'Sarkar' and directing 'The Stoneman Murders' and 'Hostel'., Categories: \"Crime Fiction\"\n",
            "4, Id: 4844, Title: Shor in the City, Summary: The film revolves around five central characters in the city of Mumbai. Tilak  is a small time publisher of pirated books who along with his friends Mandook  and Ramesh  kidnaps a famous author and forces him to give the manuscript of his latest book to them so that they will be the first ones to publish it. Abhay  is a NRI who returns to India to start his own small business and meets Shalmili . Sawan  is a young cricketer hoping to break into the under-22 Mumbai cricket team. The story focuses on their trials and tribulations as they battle life in the city of Mumbai during the chaotic period of the festival of Ganesh Chaturthi. The film tries to deal with concepts of chance, the constant struggle between hope and despair and self-actualization. The opening of the movie starts with the song \"Karma is a bitch\" which fits in with the central theme of the story where the characters of the film keep trying to come to terms with their own actions. Abhay has a dark past which is not revealed in the film but it can be speculated that he came to India to stay away from it. Tilak initially holds himself responsible for the tragic accident that happens in the movie which brings him closer to his wife and viewing life from a different perspective. The goons who traumatize Abhay eventually end up being shot by their own bullets on the day of Ganesha Visarjan indicating to an extent the influence of \"God\" in the movie. Tilak gets a new life and finds his treasure at home. Sawan who is financially pressed decides to keep the money he finds at the bank robbery and gives up on the cricket selections- a choice he could have well chosen not to take. Towards the end of the movie all characters move on different paths., Categories: \"Crime Fiction\",  \"Drama\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IvHkTUHyu78"
      },
      "source": [
        "## Βελτιστοποίηση\n",
        "\n",
        "Αφού υλοποιήσετε τη συνάρτηση `content_recommender` χρησιμοποιήστε τη για να βελτιστοποιήσετε την `TfidfVectorizer`. Συγκεκριμένα, αρχικά μπορείτε να δείτε τι επιστρέφει το σύστημα για τυχαίες ταινίες-στόχους και για ένα μικρό `max_recommendations` (2 ή 3). Αν σε κάποιες ταινίες το σύστημα μοιάζει να επιστρέφει σημασιολογικά κοντινές ταινίες σημειώστε το `ID` τους. Δοκιμάστε στη συνέχεια να βελτιστοποιήσετε την `TfidfVectorizer` για τα συγκεκριμένα `ID` ώστε να επιστρέφονται σημασιολογικά κοντινές ταινίες για μεγαλύτερο αριθμό `max_recommendations`. Παράλληλα, όσο βελτιστοποιείτε την `TfidfVectorizer`, θα πρέπει να λαμβάνετε καλές συστάσεις για μεγαλύτερο αριθμό τυχαίων ταινιών. Μπορείτε επίσης να βελτιστοποιήσετε τη συνάρτηση παρατηρώντας πολλά φαινόμενα που το σύστημα εκλαμβάνει ως ομοιότητα περιεχομένου ενώ επί της ουσίας δεν είναι επιθυμητό να συνυπολογίζονται (δείτε σχετικά το [FAQ](https://docs.google.com/document/d/1hou1gWXQuHAB7J2aV44xm_CtAWJ63q6Cu1V6OwyL_n0/edit?usp=sharing)). Ταυτόχρονα, μια άλλη κατεύθυνση της βελτιστοποίησης είναι να χρησιμοποιείτε τις παραμέτρους του `TfidfVectorizer` έτσι ώστε να μειώνονται οι διαστάσεις του Vector Space Model μέχρι το σημείο που θα αρχίσει να εμφανίζονται επιπτώσεις στην ποιότητα των συστάσεων. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPVK7Z5c1p5F"
      },
      "source": [
        "## Επεξήγηση επιλογών και ποιοτική ερμηνεία\n",
        "\n",
        "Σε markdown περιγράψτε πώς προχωρήσατε στις επιλογές σας για τη βελτιστοποίηση της `TfidfVectorizer`. Επίσης σε markdown δώστε 10 παραδείγματα (IDs) από τη συλλογή σας που επιστρέφουν καλά αποτελέσματα μέχρι `max_recommendations` (5 και παραπάνω) και σημειώστε συνοπτικά ποια είναι η θεματική που ενώνει τις ταινίες.\n",
        "\n",
        "Δείτε [εδώ](https://pastebin.com/raw/ZEvg5t3z) ένα παράδειγμα εξόδου του βελτιστοποιημένου συστήματος συστάσεων για την ταίνία [\"Q Planes\"](https://en.wikipedia.org/wiki/Q_Planes) με την κλήση της συνάρτησης για κάποιο seed `content_recommender(529,3)`. Είναι φανερό ότι η κοινή θεματική των ταινιών είναι τα αεροπλάνα, οι πτήσεις, οι πιλότοι, ο πόλεμος."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4irg4K-IPSym"
      },
      "source": [
        "## Tip: persistence αντικειμένων με joblib.dump\n",
        "\n",
        "H βιβλιοθήκη [joblib](https://pypi.python.org/pypi/joblib) της Python δίνει κάποιες εξαιρετικά χρήσιμες ιδιότητες στην ανάπτυξη κώδικα: pipelining, παραλληλισμό, caching και variable persistence. Τις τρεις πρώτες ιδιότητες τις είδαμε στην πρώτη άσκηση. Στην παρούσα άσκηση θα μας φανεί χρήσιμη η τέταρτη, το persistence των αντικειμένων. Συγκεκριμένα μπορούμε με:\n",
        "\n",
        "```python\n",
        "joblib.dump(my_object, 'my_object.pkl') \n",
        "```\n",
        "\n",
        "να αποθηκεύσουμε οποιοδήποτε αντικείμενο-μεταβλητή (εδώ το `my_object`) απευθείας πάνω στο filesystem ως αρχείο, το οποίο στη συνέχεια μπορούμε να ανακαλέσουμε ως εξής:\n",
        "\n",
        "```python\n",
        "my_object = joblib.load('my_object.pkl')\n",
        "```\n",
        "\n",
        "Μπορούμε έτσι να ανακαλέσουμε μεταβλητές ακόμα και αφού κλείσουμε και ξανανοίξουμε το notebook, χωρίς να χρειαστεί να ακολουθήσουμε ξανά όλα τα βήματα ένα - ένα για την παραγωγή τους, κάτι ιδιαίτερα χρήσιμο αν αυτή η διαδικασία είναι χρονοβόρα.\n",
        "\n",
        "Ας αποθηκεύσουμε το `corpus_tf_idf` και στη συνέχεια ας το ανακαλέσουμε."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aESOPYQaPSyo",
        "scrolled": true
      },
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(corpus_tf_idf, 'corpus_tf_idf.pkl') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_rAEj5ZPSy1"
      },
      "source": [
        "\n",
        "\n",
        "Μπορείτε με ένα απλό `!ls` να δείτε ότι το αρχείο `corpus_tf_idf.pkl` υπάρχει στο filesystem σας (== persistence):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhwXmTEIPSy3",
        "scrolled": true
      },
      "source": [
        "!ls -lh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cey5AbkO475S"
      },
      "source": [
        "και μπορούμε να τα διαβάσουμε με `joblib.load`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSJPTKY8PSyu"
      },
      "source": [
        "corpus_tf_idf = joblib.load('corpus_tf_idf.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHOQtO83PSy9"
      },
      "source": [
        "# Εφαρμογή 2.  Τοπολογική και σημασιολογική απεικόνιση της ταινιών με χρήση SOM\n",
        "<img src=\"https://i.imgur.com/Z4FdurD.jpg\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB_clmizPSy-"
      },
      "source": [
        "## Δημιουργία dataset\n",
        "Στη δεύτερη εφαρμογή θα βασιστούμε στις τοπολογικές ιδιότητες των Self Organizing Maps (SOM) για να φτιάξουμε ενά χάρτη (grid) δύο διαστάσεων όπου θα απεικονίζονται όλες οι ταινίες της συλλογής της ομάδας με τρόπο χωρικά συνεκτικό ως προς το περιεχόμενο και κυρίως το είδος τους (ο παραπάνω χάρτης είναι ενδεικτικός, δεν αντιστοιχεί στο dataset μας). \n",
        "\n",
        "Η `build_final_set` αρχικά μετατρέπει την αραιή αναπαράσταση tf-idf της εξόδου της `TfidfVectorizer()` σε πυκνή (η [αραιή αναπαράσταση](https://en.wikipedia.org/wiki/Sparse_matrix) έχει τιμές μόνο για τα μη μηδενικά στοιχεία). \n",
        "\n",
        "Στη συνέχεια ενώνει την πυκνή `dense_tf_idf` αναπαράσταση και τις binarized κατηγορίες `catbins` των ταινιών ως επιπλέον στήλες (χαρακτηριστικά). Συνεπώς, κάθε ταινία αναπαρίσταται στο Vector Space Model από τα χαρακτηριστικά του TFIDF και τις κατηγορίες της.\n",
        "\n",
        "Τέλος, δέχεται ένα ορισμα για το πόσες ταινίες να επιστρέψει, με default τιμή όλες τις ταινίες (5000). Αυτό είναι χρήσιμο για να μπορείτε αν θέλετε να φτιάχνετε μικρότερα σύνολα δεδομένων ώστε να εκπαιδεύεται ταχύτερα το SOM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-FDDOkQPSzA"
      },
      "source": [
        "def build_final_set(doc_limit = 5000, tf_idf_only=False):\n",
        "    # convert sparse tf_idf to dense tf_idf representation\n",
        "    dense_tf_idf = corpus_tf_idf.toarray()[0:doc_limit,:]\n",
        "    if tf_idf_only:\n",
        "        # use only tf_idf\n",
        "        final_set = dense_tf_idf\n",
        "    else:\n",
        "        # append the binary categories features horizontaly to the (dense) tf_idf features\n",
        "        final_set = np.hstack((dense_tf_idf, catbins[0:doc_limit,:]))\n",
        "        # η somoclu θέλει δεδομένα σε float32\n",
        "    return np.array(final_set, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF1B62UbPSzF"
      },
      "source": [
        "final_set = build_final_set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjvPPENS_dYL"
      },
      "source": [
        "Τυπώνουμε τις διαστάσεις του τελικού dataset μας. Χωρίς βελτιστοποίηση του TFIDF θα έχουμε περίπου 50.000 χαρακτηριστικά."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvEgNn-L-jEw"
      },
      "source": [
        "final_set.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om7PoyDVCqis"
      },
      "source": [
        "Με βάση την εμπειρία σας στην προετοιμασία των δεδομένων στην επιβλεπόμενη μάθηση, υπάρχει κάποιο βήμα προεπεξεργασίας που θα μπορούσε να εφαρμοστεί σε αυτό το dataset; "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tikdip0PSzQ"
      },
      "source": [
        "## Εκπαίδευση χάρτη SOM\n",
        "\n",
        "Θα δουλέψουμε με τη βιβλιοθήκη SOM [\"Somoclu\"](http://somoclu.readthedocs.io/en/stable/index.html). Εισάγουμε τις somoclu και matplotlib και λέμε στη matplotlib να τυπώνει εντός του notebook (κι όχι σε pop up window)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX9rzxGSPSzR"
      },
      "source": [
        "# install somoclu\n",
        "!pip install --upgrade somoclu\n",
        "# import sompoclu, matplotlib\n",
        "import somoclu\n",
        "import matplotlib\n",
        "# we will plot inside the notebook and not in separate window\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqBfn0ijPSzX"
      },
      "source": [
        "Καταρχάς διαβάστε το [function reference](http://somoclu.readthedocs.io/en/stable/reference.html) του somoclu. Θα δoυλέψουμε με χάρτη τύπου planar, παραλληλόγραμμου σχήματος νευρώνων με τυχαία αρχικοποίηση (όλα αυτά είναι default). Μπορείτε να δοκιμάσετε διάφορα μεγέθη χάρτη ωστόσο όσο ο αριθμός των νευρώνων μεγαλώνει, μεγαλώνει και ο χρόνος εκπαίδευσης. Για το training δεν χρειάζεται να ξεπεράσετε τα 100 epochs. Σε γενικές γραμμές μπορούμε να βασιστούμε στις default παραμέτρους μέχρι να έχουμε τη δυνατότητα να οπτικοποιήσουμε και να αναλύσουμε ποιοτικά τα αποτελέσματα. Ξεκινήστε με ένα χάρτη 10 x 10, 100 epochs training και ένα υποσύνολο των ταινιών (π.χ. 2000). Χρησιμοποιήστε την `time` για να έχετε μια εικόνα των χρόνων εκπαίδευσης. Ενδεικτικά, με σωστή κωδικοποίηση tf-idf, μικροί χάρτες για λίγα δεδομένα (1000-2000) παίρνουν γύρω στο ένα λεπτό ενώ μεγαλύτεροι χάρτες με όλα τα δεδομένα μπορούν να πάρουν 10-15 λεπτά ή και περισσότερο.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntd2GE9SaHiS"
      },
      "source": [
        "\n",
        "## Best matching units\n",
        "\n",
        "Μετά από κάθε εκπαίδευση αποθηκεύστε σε μια μεταβλητή τα best matching units (bmus) για κάθε ταινία. Τα bmus μας δείχνουν σε ποιο νευρώνα ανήκει η κάθε ταινία. Προσοχή: η σύμβαση των συντεταγμένων των νευρώνων είναι (στήλη, γραμμή) δηλαδή το ανάποδο από την Python. Με χρήση της [np.unique](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.unique.html) (μια πολύ χρήσιμη συνάρτηση στην άσκηση) αποθηκεύστε τα μοναδικά best matching units και τους δείκτες τους (indices) προς τις ταινίες. Σημειώστε ότι μπορεί να έχετε λιγότερα μοναδικά bmus από αριθμό νευρώνων γιατί μπορεί σε κάποιους νευρώνες να μην έχουν ανατεθεί ταινίες. Ως αριθμό νευρώνα θα θεωρήσουμε τον αριθμό γραμμής στον πίνακα μοναδικών bmus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grzqcyHyaKdg"
      },
      "source": [
        "\n",
        "## Ομαδοποίηση (clustering)\n",
        "\n",
        "Τυπικά, η ομαδοποίηση σε ένα χάρτη SOM προκύπτει από το unified distance matrix (U-matrix): για κάθε κόμβο υπολογίζεται η μέση απόστασή του από τους γειτονικούς κόμβους. Εάν χρησιμοποιηθεί μπλε χρώμα στις περιοχές του χάρτη όπου η τιμή αυτή είναι χαμηλή (μικρή απόσταση) και κόκκινο εκεί που η τιμή είναι υψηλή (μεγάλη απόσταση), τότε μπορούμε να πούμε ότι οι μπλε περιοχές αποτελούν clusters και οι κόκκινες αποτελούν σύνορα μεταξύ clusters.\n",
        "\n",
        "To somoclu δίνει την επιπρόσθετη δυνατότητα να κάνουμε ομαδοποίηση των νευρώνων χρησιμοποιώντας οποιονδήποτε αλγόριθμο ομαδοποίησης του scikit-learn. Στην άσκηση θα χρησιμοποιήσουμε τον k-Means. Για τον αρχικό σας χάρτη δοκιμάστε ένα k=20 ή 25. Οι δύο προσεγγίσεις ομαδοποίησης είναι διαφορετικές, οπότε περιμένουμε τα αποτελέσματα να είναι κοντά αλλά όχι τα ίδια.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nupuqcuaMe3"
      },
      "source": [
        "\n",
        "## Αποθήκευση του SOM\n",
        "\n",
        "Επειδή η αρχικοποίηση του SOM γίνεται τυχαία και το clustering είναι και αυτό στοχαστική διαδικασία, οι θέσεις και οι ετικέτες των νευρώνων και των clusters θα είναι διαφορετικές κάθε φορά που τρέχετε τον χάρτη, ακόμα και με τις ίδιες παραμέτρους. Για να αποθηκεύσετε ένα συγκεκριμένο som και clustering χρησιμοποιήστε και πάλι την `joblib`. Μετά την ανάκληση ενός SOM θυμηθείτε να ακολουθήσετε τη διαδικασία για τα bmus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejX0Qs18aRHU"
      },
      "source": [
        "\n",
        "## Οπτικοποίηση U-matrix, clustering και μέγεθος clusters\n",
        "\n",
        "Για την εκτύπωση του U-matrix χρησιμοποιήστε τη `view_umatrix` με ορίσματα `bestmatches=True` και `figsize=(15, 15)` ή `figsize=(20, 20)`. Τα διαφορετικά χρώματα που εμφανίζονται στους κόμβους αντιπροσωπεύουν τα διαφορετικά clusters που προκύπτουν από τον k-Means. Μπορείτε να εμφανίσετε τη λεζάντα του U-matrix με το όρισμα `colorbar`. Μην τυπώνετε τις ετικέτες (labels) των δειγμάτων, είναι πολύ μεγάλος ο αριθμός τους.\n",
        "\n",
        "Για μια δεύτερη πιο ξεκάθαρη οπτικοποίηση του clustering τυπώστε απευθείας τη μεταβλητή `clusters`.\n",
        "\n",
        "Τέλος, χρησιμοποιώντας πάλι την `np.unique` (με διαφορετικό όρισμα) και την `np.argsort` (υπάρχουν και άλλοι τρόποι υλοποίησης) εκτυπώστε τις ετικέτες των clusters (αριθμοί από 0 έως k-1) και τον αριθμό των νευρώνων σε κάθε cluster, με φθίνουσα ή αύξουσα σειρά ως προς τον αριθμό των νευρώνων. Ουσιαστικά είναι ένα εργαλείο για να βρίσκετε εύκολα τα μεγάλα και μικρά clusters. \n",
        "\n",
        "Ακολουθεί ένα μη βελτιστοποιημένο παράδειγμα για τις τρεις προηγούμενες εξόδους:\n",
        "\n",
        "<img src=\"https://image.ibb.co/i0tsfR/umatrix_s.jpg\" width=\"35%\">\n",
        "<img src=\"https://image.ibb.co/nLgHEm/clusters.png\" width=\"35%\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMO_KcQYaTv-"
      },
      "source": [
        "\n",
        "## Σημασιολογική ερμηνεία των clusters\n",
        "\n",
        "Προκειμένου να μελετήσουμε τις τοπολογικές ιδιότητες του SOM και το αν έχουν ενσωματώσει σημασιολογική πληροφορία για τις ταινίες διαμέσου της διανυσματικής αναπαράστασης με το tf-idf και των κατηγοριών, χρειαζόμαστε ένα κριτήριο ποιοτικής επισκόπησης των clusters. Θα υλοποιήσουμε το εξής κριτήριο: Λαμβάνουμε όρισμα έναν αριθμό (ετικέτα) cluster. Για το cluster αυτό βρίσκουμε όλους τους νευρώνες που του έχουν ανατεθεί από τον k-Means. Για όλους τους νευρώνες αυτούς βρίσκουμε όλες τις ταινίες που τους έχουν ανατεθεί (για τις οποίες αποτελούν bmus). Για όλες αυτές τις ταινίες τυπώνουμε ταξινομημένη τη συνολική στατιστική όλων των ειδών (κατηγοριών) και τις συχνότητές τους. Αν το cluster διαθέτει καλή συνοχή και εξειδίκευση, θα πρέπει κάποιες κατηγορίες να έχουν σαφώς μεγαλύτερη συχνότητα από τις υπόλοιπες. Θα μπορούμε τότε να αναθέσουμε αυτήν/ές την/τις κατηγορία/ες ως ετικέτες κινηματογραφικού είδους στο cluster.\n",
        "\n",
        "Μπορείτε να υλοποιήσετε τη συνάρτηση αυτή όπως θέλετε. Μια πιθανή διαδικασία θα μπορούσε να είναι η ακόλουθη:\n",
        "\n",
        "1. Ορίζουμε συνάρτηση `print_categories_stats` που δέχεται ως είσοδο λίστα με ids ταινιών. Δημιουργούμε μια κενή λίστα συνολικών κατηγοριών. Στη συνέχεια, για κάθε ταινία επεξεργαζόμαστε το string `categories` ως εξής: δημιουργούμε μια λίστα διαχωρίζοντας το string κατάλληλα με την `split` και αφαιρούμε τα whitespaces μεταξύ ετικετών με την `strip`. Προσθέτουμε τη λίστα αυτή στη συνολική λίστα κατηγοριών με την `extend`. Τέλος χρησιμοποιούμε πάλι την `np.unique` για να μετρήσουμε συχνότητα μοναδικών ετικετών κατηγοριών και ταξινομούμε με την `np.argsort`. Τυπώνουμε τις κατηγορίες και τις συχνότητες εμφάνισης ταξινομημένα. Χρήσιμες μπορεί να σας φανούν και οι `np.ravel`, `np.nditer`, `np.array2string` και `zip`.\n",
        "\n",
        "2. Ορίζουμε τη βασική μας συνάρτηση `print_cluster_neurons_movies_report` που δέχεται ως όρισμα τον αριθμό ενός cluster. Με τη χρήση της `np.where` μπορούμε να βρούμε τις συντεταγμένες των bmus που αντιστοιχούν στο cluster και με την `column_stack` να φτιάξουμε έναν πίνακα bmus για το cluster. Προσοχή στη σειρά (στήλη - σειρά) στον πίνακα bmus. Για κάθε bmu αυτού του πίνακα ελέγχουμε αν υπάρχει στον πίνακα μοναδικών bmus που έχουμε υπολογίσει στην αρχή συνολικά και αν ναι προσθέτουμε το αντίστοιχο index του νευρώνα σε μια λίστα. Χρήσιμες μπορεί να είναι και οι `np.rollaxis`, `np.append`, `np.asscalar`. Επίσης πιθανώς να πρέπει να υλοποιήσετε ένα κριτήριο ομοιότητας μεταξύ ενός bmu και ενός μοναδικού bmu από τον αρχικό πίνακα bmus.\n",
        "\n",
        "3. Υλοποιούμε μια βοηθητική συνάρτηση `neuron_movies_report`. Λαμβάνει ένα σύνολο νευρώνων από την `print_cluster_neurons_movies_report` και μέσω της `indices` φτιάχνει μια λίστα με το σύνολο ταινιών που ανήκουν σε αυτούς τους νευρώνες. Στο τέλος καλεί με αυτή τη λίστα την `print_categories_stats` που τυπώνει τις στατιστικές των κατηγοριών.\n",
        "\n",
        "Μπορείτε βέβαια να προσθέσετε οποιαδήποτε επιπλέον έξοδο σας βοηθάει. Μια χρήσιμη έξοδος είναι πόσοι νευρώνες ανήκουν στο cluster και σε πόσους και ποιους από αυτούς έχουν ανατεθεί ταινίες.\n",
        "\n",
        "Θα επιτελούμε τη σημασιολογική ερμηνεία του χάρτη καλώντας την `print_cluster_neurons_movies_report` με τον αριθμός ενός cluster που μας ενδιαφέρει. \n",
        "\n",
        "Παράδειγμα εξόδου για ένα cluster (μη βελτιστοποιημένος χάρτης, ωστόσο βλέπετε ότι οι μεγάλες κατηγορίες έχουν σημασιολογική  συνάφεια):\n",
        "\n",
        "```\n",
        "Overall Cluster Genres stats:  \n",
        "[('\"Horror\"', 86), ('\"Science Fiction\"', 24), ('\"B-movie\"', 16), ('\"Monster movie\"', 10), ('\"Creature Film\"', 10), ('\"Indie\"', 9), ('\"Zombie Film\"', 9), ('\"Slasher\"', 8), ('\"World cinema\"', 8), ('\"Sci-Fi Horror\"', 7), ('\"Natural horror films\"', 6), ('\"Supernatural\"', 6), ('\"Thriller\"', 6), ('\"Cult\"', 5), ('\"Black-and-white\"', 5), ('\"Japanese Movies\"', 4), ('\"Short Film\"', 3), ('\"Drama\"', 3), ('\"Psychological thriller\"', 3), ('\"Crime Fiction\"', 3), ('\"Monster\"', 3), ('\"Comedy\"', 2), ('\"Western\"', 2), ('\"Horror Comedy\"', 2), ('\"Archaeology\"', 2), ('\"Alien Film\"', 2), ('\"Teen\"', 2), ('\"Mystery\"', 2), ('\"Adventure\"', 2), ('\"Comedy film\"', 2), ('\"Combat Films\"', 1), ('\"Chinese Movies\"', 1), ('\"Action/Adventure\"', 1), ('\"Gothic Film\"', 1), ('\"Costume drama\"', 1), ('\"Disaster\"', 1), ('\"Docudrama\"', 1), ('\"Film adaptation\"', 1), ('\"Film noir\"', 1), ('\"Parody\"', 1), ('\"Period piece\"', 1), ('\"Action\"', 1)]```\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq4QrImhaa7E"
      },
      "source": [
        "\n",
        "## Tips για το SOM και το clustering\n",
        "\n",
        "- Για την ομαδοποίηση ένα U-matrix καλό είναι να εμφανίζει και μπλε-πράσινες περιοχές (clusters) και κόκκινες περιοχές (ορίων). Παρατηρήστε ποια σχέση υπάρχει μεταξύ αριθμού ταινιών στο final set, μεγέθους grid και ποιότητας U-matrix.\n",
        "- Για το k του k-Means προσπαθήστε να προσεγγίζει σχετικά τα clusters του U-matrix (όπως είπαμε είναι διαφορετικοί μέθοδοι clustering). Μικρός αριθμός k δεν θα σέβεται τα όρια. Μεγάλος αριθμός θα δημιουργεί υπο-clusters εντός των clusters που φαίνονται στο U-matrix. Το τελευταίο δεν είναι απαραίτητα κακό, αλλά μεγαλώνει τον αριθμό clusters που πρέπει να αναλυθούν σημασιολογικά.\n",
        "- Σε μικρούς χάρτες και με μικρά final sets δοκιμάστε διαφορετικές παραμέτρους για την εκπαίδευση του SOM. Σημειώστε τυχόν παραμέτρους που επηρεάζουν την ποιότητα του clustering για το dataset σας ώστε να τις εφαρμόσετε στους μεγάλους χάρτες.\n",
        "- Κάποια τοπολογικά χαρακτηριστικά εμφανίζονται ήδη σε μικρούς χάρτες. Κάποια άλλα χρειάζονται μεγαλύτερους χάρτες. Δοκιμάστε μεγέθη 20x20, 25x25 ή και 30x30 και αντίστοιχη προσαρμογή των k. Όσο μεγαλώνουν οι χάρτες, μεγαλώνει η ανάλυση του χάρτη αλλά μεγαλώνει και ο αριθμός clusters που πρέπει να αναλυθούν.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4IUl8O8ayVf"
      },
      "source": [
        "\n",
        "\n",
        "## Ανάλυση τοπολογικών ιδιοτήτων χάρτη SOM\n",
        "\n",
        "Μετά το πέρας της εκπαίδευσης και του clustering θα έχετε ένα χάρτη με τοπολογικές ιδιότητες ως προς τα είδη των ταίνιών της συλλογής σας, κάτι αντίστοιχο με την εικόνα στην αρχή της Εφαρμογής 2 αυτού του notebook (η συγκεκριμένη εικόνα είναι μόνο για εικονογράφιση, δεν έχει καμία σχέση με τη συλλογή δεδομένων και τις κατηγορίες μας).\n",
        "\n",
        "Για τον τελικό χάρτη SOM που θα παράξετε για τη συλλογή σας, αναλύστε σε markdown με συγκεκριμένη αναφορά σε αριθμούς clusters και τη σημασιολογική ερμηνεία τους τις εξής τρεις τοπολογικές ιδιότητες του SOM: \n",
        "\n",
        "1. Δεδομένα που έχουν μεγαλύτερη πυκνότητα πιθανότητας στο χώρο εισόδου τείνουν να απεικονίζονται με περισσότερους νευρώνες στο χώρο μειωμένης διαστατικότητας. Δώστε παραδείγματα από συχνές και λιγότερο συχνές κατηγορίες ταινιών. Χρησιμοποιήστε τις στατιστικές των κατηγοριών στη συλλογή σας και τον αριθμό κόμβων που χαρακτηρίζουν.\n",
        "2. Μακρινά πρότυπα εισόδου τείνουν να απεικονίζονται απομακρυσμένα στο χάρτη. Υπάρχουν χαρακτηριστικές κατηγορίες ταινιών που ήδη από μικρούς χάρτες τείνουν να τοποθετούνται σε διαφορετικά ή απομονωμένα σημεία του χάρτη.\n",
        "3. Κοντινά πρότυπα εισόδου τείνουν να απεικονίζονται κοντά στο χάρτη. Σε μεγάλους χάρτες εντοπίστε είδη ταινιών και κοντινά τους υποείδη.\n",
        "\n",
        "Προφανώς τοποθέτηση σε 2 διαστάσεις που να σέβεται μια απόλυτη τοπολογία δεν είναι εφικτή, αφενός γιατί δεν υπάρχει κάποια απόλυτη εξ ορισμού για τα κινηματογραφικά είδη ακόμα και σε πολλές διαστάσεις, αφετέρου γιατί πραγματοποιούμε μείωση διαστατικότητας.\n",
        "\n",
        "Εντοπίστε μεγάλα clusters και μικρά clusters που δεν έχουν σαφή χαρακτηριστικά. Εντοπίστε clusters συγκεκριμένων ειδών που μοιάζουν να μην έχουν τοπολογική συνάφεια με γύρω περιοχές. Προτείνετε πιθανές ερμηνείες.\n",
        "\n",
        "\n",
        "\n",
        "Τέλος, εντοπίστε clusters που έχουν κατά την άποψή σας ιδιαίτερο ενδιαφέρον στη συλλογή της ομάδας σας (data exploration / discovery value) και σχολιάστε.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYjxGR5DawIy"
      },
      "source": [
        "\n",
        "# Τελική παράδοση άσκησης\n",
        "\n",
        "- Θα παραδώσετε στο eclass το παρόν notebook επεξεργασμένο ή ένα νέο με τις απαντήσεις σας για τα ζητούμενα και των δύο εφαρμογών. \n",
        "- Θυμηθείτε ότι η ανάλυση του χάρτη στο markdown με αναφορά σε αριθμούς clusters πρέπει να αναφέρεται στον τελικό χάρτη με τα κελιά ορατά που θα παραδώσετε αλλιώς ο χάρτης που θα προκύψει θα είναι διαφορετικός και τα labels των clusters δεν θα αντιστοιχούν στην ανάλυσή σας. \n",
        "- Μην ξεχάσετε στην αρχή ένα κελί markdown με **τα στοιχεία της ομάδας σας**.\n",
        "- Στο **zip** που θα παραδώσετε πρέπει να βρίσκονται **2 αρχεία (το .ipynb και το .py του notebook σας)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHhCkvxjnitd"
      },
      "source": [
        "<table>\n",
        "  <tr><td align=\"center\">\n",
        "    <font size=\"4\">Παρακαλούμε διατρέξτε βήμα-βήμα το notebook για να μην ξεχάσετε παραδοτέα!</font>\n",
        "</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    }
  ]
}