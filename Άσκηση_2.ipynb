{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prev.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.5",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLFXD3IePSyL"
      },
      "source": [
        "# Εργαστηριακή Άσκηση 2. Μη επιβλεπόμενη μάθηση. \n",
        "## Σύστημα συστάσεων βασισμένο στο περιεχόμενο\n",
        "## Σημασιολογική απεικόνιση δεδομένων με χρήση SOM \n",
        "Ημερομηνία εκφώνησης της άσκησης: 23 Νοεμβρίου 2020\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQwvNaBXatQc"
      },
      "source": [
        "### **Στοιχεία ομάδας**\r\n",
        "#### Ομάδα 15\r\n",
        "#### Μαρίνος Τζανίκος 03113147, el13147@mail.ntua.gr\r\n",
        "#### Γιώργος Γκανάς 03116095, el16095@mail.ntua.gr\r\n",
        "#### Παναγιώτης Αϊβασιλιώτης 03116176, el16176@mail.ntua.gr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5wbBzIYnird",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceec6604-899e-448d-be20-b79bb2623678"
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade pandas\n",
        "!pip install --upgrade nltk\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install --upgrade joblib"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (20.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.19.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.19.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.24.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (2.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.0.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.19.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aViHqlQcPSyP"
      },
      "source": [
        "## Εισαγωγή του Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZVmdDExPSyQ"
      },
      "source": [
        "Το σύνολο δεδομένων με το οποίο θα δουλέψουμε είναι βασισμένο στο [Carnegie Mellon Movie Summary Corpus](http://www.cs.cmu.edu/~ark/personas/). Πρόκειται για ένα dataset με περίπου 40.000 περιγραφές ταινιών. Η περιγραφή κάθε ταινίας αποτελείται από τον τίτλο της, μια ή περισσότερες ετικέτες που χαρακτηρίζουν το είδος της ταινίας και τέλος τη σύνοψη της υπόθεσής της. Αρχικά εισάγουμε το dataset (χρησιμοποιήστε αυτούσιο τον κώδικα, δεν χρειάζεστε το αρχείο csv) στο dataframe `df_data_1`: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62SOj46gPSyS"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset_url = \"https://drive.google.com/uc?export=download&id=1PdkVDENX12tQliCk_HtUnAUbfxXvnWuG\"\n",
        "df_data_1 = pd.read_csv(dataset_url, sep='\\t',  header=None, quoting=3, error_bad_lines=False)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TAEZGdIPSyW"
      },
      "source": [
        "Κάθε ομάδα θα δουλέψει σε ένα μοναδικό υποσύνολο 5.000 ταινιών (διαφορετικό dataset για κάθε ομάδα) ως εξής\n",
        "\n",
        "1. Κάθε ομάδα έχει έναν αριθμό \"seed\" (φύτρο) που είναι ο ίδιος με τον αριθμό της ομάδας σας: θα τον βρείτε στην κολόνα Α/Α [εδώ](https://docs.google.com/spreadsheets/d/1CD6AtX7YnocXceCELl_XJ06kyRr0YQPhor8dpw012t0/edit?usp=sharing).\n",
        "\n",
        "2. Το data frame `df_data_2` έχει γραμμές όσες και οι ομάδες και 5.000 στήλες. Σε κάθε ομάδα αντιστοιχεί η γραμμή του πίνακα με το `team_seed_number` της. Η γραμμή αυτή θα περιλαμβάνει 5.000 διαφορετικούς αριθμούς που αντιστοιχούν σε ταινίες του αρχικού dataset. \n",
        "\n",
        "3. Στο επόμενο κελί αλλάξτε τη μεταβλητή `team_seed_number` με το Seed της ομάδας σας.\n",
        "\n",
        "4. Τρέξτε τον κώδικα. Θα προκύψουν τα μοναδικά για κάθε ομάδα  titles, categories, catbins, summaries και corpus με τα οποία θα δουλέψετε."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2POlqDjkPSyY"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# βάλτε το seed που αντιστοιχεί στην ομάδα σας\n",
        "team_seed_number = 15\n",
        "\n",
        "movie_seeds_url = \"https://drive.google.com/uc?export=download&id=1EA_pUIgK5Ub3kEzFbFl8wSRqAV6feHqD\"\n",
        "df_data_2 = pd.read_csv(movie_seeds_url, header=None, error_bad_lines=False)\n",
        "\n",
        "# επιλέγεται \n",
        "my_index = df_data_2.iloc[team_seed_number,:].values\n",
        "\n",
        "titles = df_data_1.iloc[:, [2]].values[my_index] # movie titles (string)\n",
        "categories = df_data_1.iloc[:, [3]].values[my_index] # movie categories (string)\n",
        "bins = df_data_1.iloc[:, [4]]\n",
        "catbins = bins[4].str.split(',', expand=True).values.astype(np.float)[my_index] # movie categories in binary form (1 feature per category)\n",
        "summaries =  df_data_1.iloc[:, [5]].values[my_index] # movie summaries (string)\n",
        "corpus = summaries[:,0].tolist() # list form of summaries"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If66lkwxPSyb"
      },
      "source": [
        "- Ο πίνακας **titles** περιέχει τους τίτλους των ταινιών. Παράδειγμα: 'Sid and Nancy'.\n",
        "- O πίνακας **categories** περιέχει τις κατηγορίες (είδη) της ταινίας υπό τη μορφή string. Παράδειγμα: '\"Tragedy\",  \"Indie\",  \"Punk rock\",  \"Addiction Drama\",  \"Cult\",  \"Musical\",  \"Drama\",  \"Biopic \\[feature\\]\",  \"Romantic drama\",  \"Romance Film\",  \"Biographical film\"'. Παρατηρούμε ότι είναι μια comma separated λίστα strings, με κάθε string να είναι μια κατηγορία.\n",
        "- Ο πίνακας **catbins** περιλαμβάνει πάλι τις κατηγορίες των ταινιών αλλά σε δυαδική μορφή ([one hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)). Έχει διαστάσεις 5.000 x 322 (όσες οι διαφορετικές κατηγορίες). Αν η ταινία ανήκει στο συγκεκριμένο είδος η αντίστοιχη στήλη παίρνει την τιμή 1, αλλιώς παίρνει την τιμή 0.\n",
        "- Ο πίνακας **summaries** και η λίστα **corpus** περιλαμβάνουν τις συνόψεις των ταινιών (η corpus είναι απλά ο summaries σε μορφή λίστας). Κάθε σύνοψη είναι ένα (συνήθως μεγάλο) string. Παράδειγμα: *'The film is based on the real story of a Soviet Internal Troops soldier who killed his entire unit  as a result of Dedovschina. The plot unfolds mostly on board of the prisoner transport rail car guarded by a unit of paramilitary conscripts.'*\n",
        "- Θεωρούμε ως **ID** της κάθε ταινίας τον αριθμό γραμμής της ή το αντίστοιχο στοιχείο της λίστας. Παράδειγμα: για να τυπώσουμε τη σύνοψη της ταινίας με `ID=99` (την εκατοστή) θα γράψουμε `print(corpus[99])`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_7A3KXLp0qS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a17bcc-c3f2-4759-ca58-11dd488fccbb"
      },
      "source": [
        "ID = 99\n",
        "print(titles[ID])\n",
        "print(categories[ID])\n",
        "print(catbins[ID])\n",
        "print(corpus[ID])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Congkak']\n",
            "['\"Horror\"']\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Kazman is a dedicated husband who, despite his wife Sufiah's protests, decided to buy a bungalow for their family near a lake as a family getaway. Anxious in the new house, Sufiah feels as if someone is watching her. Her daughter Lisa, who would always go downstairs at night to play congkak with someone whom she could only see, compounds her uneasiness. Sufiah throws the congkak in the lake, but is awakened the next night by the sound of the congkak being played again and upon investigation, she sees an old lady playing it. When her daughter disappears, the oldest resident in that area, Pak Tua, comes to the family's rescue and helps in locating the missing family members.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTNgwBfjPSyc"
      },
      "source": [
        "# Εφαρμογή 1. Υλοποίηση συστήματος συστάσεων ταινιών βασισμένο στο περιεχόμενο\n",
        "<img src=\"http://clture.org/wp-content/uploads/2015/12/Netflix-Streaming-End-of-Year-Posts.jpg\" width=\"70%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnA2RP8GPSyf"
      },
      "source": [
        "Η πρώτη εφαρμογή που θα αναπτύξετε θα είναι ένα [σύστημα συστάσεων](https://en.wikipedia.org/wiki/Recommender_system) ταινιών βασισμένο στο περιεχόμενο (content based recommender system). Τα συστήματα συστάσεων στοχεύουν στο να προτείνουν αυτόματα στο χρήστη αντικείμενα από μια συλλογή τα οποία ιδανικά θέλουμε να βρει ενδιαφέροντα ο χρήστης. Η κατηγοριοποίηση των συστημάτων συστάσεων βασίζεται στο πώς γίνεται η επιλογή (filtering) των συστηνόμενων αντικειμένων. Οι δύο κύριες κατηγορίες είναι η συνεργατική διήθηση (collaborative filtering) όπου το σύστημα προτείνει στο χρήστη αντικείμενα που έχουν αξιολογηθεί θετικά από χρήστες που έχουν παρόμοιο με αυτόν ιστορικό αξιολογήσεων και η διήθηση με βάση το περιεχόμενο (content based filtering), όπου προτείνονται στο χρήστη αντικείμενα με παρόμοιο περιεχόμενο (με βάση κάποια χαρακτηριστικά) με αυτά που έχει προηγουμένως αξιολογήσει θετικά.\n",
        "\n",
        "Το σύστημα συστάσεων που θα αναπτύξετε θα βασίζεται στο **περιεχόμενο** και συγκεκριμένα στις συνόψεις των ταινιών (corpus). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD5KuSKrxQ8I"
      },
      "source": [
        "## Μετατροπή σε TFIDF\n",
        "\n",
        "Το πρώτο βήμα θα είναι λοιπόν να μετατρέψετε το corpus σε αναπαράσταση tf-idf:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5YP6XCZPSyh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1778b9-73e5-4389-ebfd-c7a1187274c7"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('names')\n",
        "from nltk.corpus import stopwords, names\n",
        "\n",
        "\n",
        "def preprocess(s) : \n",
        "  stemmer = nltk.stem.PorterStemmer()\n",
        "  #lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  s = (s.lower()).translate(table)\n",
        "  s = re.sub(r'\\d+', 'num', s)\n",
        "  words = s.split()\n",
        "  return \" \".join([stemmer.stem(word) for word in words])\n",
        "  #return \" \".join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "stopwords = stopwords.words('english') + list(string.punctuation) + names.words()\n",
        "\n",
        "preprocessed_stopwords = preprocess(\" \".join(stopwords)).split() + ['becau']\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zJujXF0bmxH"
      },
      "source": [
        "Στο βήμα αυτό ορίζουμε τη συνάρτηση `preprocess` που θα χρησιμοποιήσουμε στα δεδομένα για τη μετατροπή TFIDF καθώς και τις stopwords.<br/>\r\n",
        "Η προεπεξεργασία εμπεριέχει την εφαρμογή Stemmer ώστε να μη θεωρούνται ξεχωριστές στο χώρο TFIDF διαφορετικές μορφές της ίδιας λέξης, καθώς και μετατροπή σε πεζά γράμματα για τη σωστή λειτουργία του vectorizer. Δοκιμάστηκε και lemmatizer αντί του stemmer, αλλά τα καλύτερα αποτελέσματα παρατηρήθηκαν με τη χρήση stemmer.<br/>\r\n",
        "\r\n",
        "Για τις stopwords χρησιμοποιούμε την αντίστοιχη λίστα που περιέχει για την αγγλική γλώσσα το nltk, σημεία στίξης, καθώς και λίστα ονομάτων που παρέχει το nltk. Το τελευταίο γίνεται ώστε να αποφευχθούν περιπτώσεις όπου ταινίες θεωρούνται όμοιες λόγω της ύπαρξης κοινών (αγγλικών) ονομάτων."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiaoxLjtk8IR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a6f525-46fa-4f23-fad8-bee7e6d4bd04"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "vectorizer = TfidfVectorizer(min_df=0.003, max_df=0.025, max_features=2000,\r\n",
        "                             stop_words=preprocessed_stopwords, preprocessor=preprocess)\r\n",
        "vectorizer.fit(corpus)\r\n",
        "corpus_tf_idf = vectorizer.transform(corpus)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abig', 'ainsl', 'alfon', 'aloi', 'alphon', 'anal', 'anali', 'annal', 'annali', 'arl', 'ashl', 'aubr', 'aur', 'beren', 'beverl', 'blakel', 'bonn', 'carr', 'catl', 'cind', 'cord', 'cort', 'darc', 'deed', 'desar', 'desir', 'dorol', 'doroth', 'dorr', 'ebon', 'el', 'eloi', 'emal', 'emmal', 'gerril', 'godfr', 'goo', 'guenev', 'guinev', 'hail', 'heli', 'heloi', 'horten', 'huntl', 'il', 'ili', 'jenil', 'jennil', 'jeral', 'jerril', 'jerryl', 'karal', 'kayc', 'kimberl', 'kirb', 'kiss', 'kizz', 'loral', 'loril', 'magg', 'maril', 'marl', 'marril', 'maryro', 'meli', 'melod', 'merc', 'merral', 'moi', 'mor', 'morl', 'oph', 'ore', 'rival', 'sall', 'salvat', 'shand', 'sheer', 'sidn', 'sind', 'spen', 'tere', 'timoth', 'valar', 'zebed'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_Cw0brpnisF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8d24d9-d57f-44bf-aabc-9b38eca1bf54"
      },
      "source": [
        "print(corpus_tf_idf.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 2000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0REonkld4RS"
      },
      "source": [
        "Οι παραπάνω τιμές των παραμέτρων του `TfidVectorizer` προέκυψαν κατά τη βελτιστοποίηση του συστήματος προτάσεων και θα σχολιαστούν παρακάτω στην αντίστοιχη ενότητα."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7pzfu6tXOXV"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "\r\n",
        "vec = CountVectorizer().fit(corpus)\r\n",
        "bag_of_words = vec.transform(corpus)\r\n",
        "sum_words = bag_of_words.sum(axis=0) \r\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\r\n",
        "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABpmcj5MXdVc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d727ac-239c-48b7-8b33-18eff027fcfc"
      },
      "source": [
        "words_freq[120:150]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('again', 1154),\n",
              " ('if', 1149),\n",
              " ('becomes', 1134),\n",
              " ('leaves', 1130),\n",
              " ('soon', 1116),\n",
              " ('friends', 1108),\n",
              " ('comes', 1100),\n",
              " ('years', 1099),\n",
              " ('men', 1086),\n",
              " ('does', 1080),\n",
              " ('leave', 1056),\n",
              " ('girl', 1046),\n",
              " ('school', 1032),\n",
              " ('see', 1023),\n",
              " ('named', 1014),\n",
              " ('finally', 1013),\n",
              " ('asks', 1000),\n",
              " ('three', 998),\n",
              " ('meanwhile', 995),\n",
              " ('kill', 987),\n",
              " ('end', 984),\n",
              " ('brother', 982),\n",
              " ('eventually', 966),\n",
              " ('room', 961),\n",
              " ('people', 961),\n",
              " ('having', 952),\n",
              " ('each', 952),\n",
              " ('group', 952),\n",
              " ('returns', 949),\n",
              " ('town', 946)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1iepgEXxzDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "968b68fb-0437-47f5-baea-6a0476e98eaa"
      },
      "source": [
        "vectorizer.get_feature_names()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['abduct',\n",
              " 'abil',\n",
              " 'aboard',\n",
              " 'abort',\n",
              " 'abroad',\n",
              " 'abruptli',\n",
              " 'absenc',\n",
              " 'academi',\n",
              " 'access',\n",
              " 'accomplic',\n",
              " 'accomplish',\n",
              " 'accord',\n",
              " 'account',\n",
              " 'achiev',\n",
              " 'acquaint',\n",
              " 'acquir',\n",
              " 'actor',\n",
              " 'actress',\n",
              " 'ad',\n",
              " 'adapt',\n",
              " 'add',\n",
              " 'addict',\n",
              " 'addit',\n",
              " 'address',\n",
              " 'adjust',\n",
              " 'admir',\n",
              " 'adult',\n",
              " 'advantag',\n",
              " 'adventur',\n",
              " 'advertis',\n",
              " 'advic',\n",
              " 'advis',\n",
              " 'afford',\n",
              " 'afraid',\n",
              " 'africa',\n",
              " 'african',\n",
              " 'afternoon',\n",
              " 'agenc',\n",
              " 'aggress',\n",
              " 'ahead',\n",
              " 'aircraft',\n",
              " 'airplan',\n",
              " 'airport',\n",
              " 'alarm',\n",
              " 'alcohol',\n",
              " 'alert',\n",
              " 'alien',\n",
              " 'alleg',\n",
              " 'alter',\n",
              " 'altern',\n",
              " 'amaz',\n",
              " 'ambit',\n",
              " 'ambiti',\n",
              " 'ambush',\n",
              " 'amongst',\n",
              " 'amount',\n",
              " 'amus',\n",
              " 'ancient',\n",
              " 'anderson',\n",
              " 'angrili',\n",
              " 'anniversari',\n",
              " 'annoy',\n",
              " 'anonym',\n",
              " 'anxiou',\n",
              " 'anymor',\n",
              " 'anyway',\n",
              " 'appeal',\n",
              " 'appli',\n",
              " 'appoint',\n",
              " 'appreci',\n",
              " 'apprehend',\n",
              " 'approv',\n",
              " 'architect',\n",
              " 'argu',\n",
              " 'argument',\n",
              " 'aris',\n",
              " 'armor',\n",
              " 'arrog',\n",
              " 'arrow',\n",
              " 'articl',\n",
              " 'artifact',\n",
              " 'asid',\n",
              " 'asleep',\n",
              " 'aspir',\n",
              " 'assail',\n",
              " 'assassin',\n",
              " 'assault',\n",
              " 'assembl',\n",
              " 'associ',\n",
              " 'assur',\n",
              " 'asylum',\n",
              " 'athlet',\n",
              " 'atmospher',\n",
              " 'atom',\n",
              " 'attach',\n",
              " 'attic',\n",
              " 'attitud',\n",
              " 'attorney',\n",
              " 'audit',\n",
              " 'aunt',\n",
              " 'australia',\n",
              " 'australian',\n",
              " 'auto',\n",
              " 'avail',\n",
              " 'aveng',\n",
              " 'await',\n",
              " 'awak',\n",
              " 'awaken',\n",
              " 'awar',\n",
              " 'award',\n",
              " 'awkward',\n",
              " 'axe',\n",
              " 'babu',\n",
              " 'background',\n",
              " 'badli',\n",
              " 'bag',\n",
              " 'bail',\n",
              " 'bait',\n",
              " 'baker',\n",
              " 'balanc',\n",
              " 'balconi',\n",
              " 'ball',\n",
              " 'ballet',\n",
              " 'balloon',\n",
              " 'bandit',\n",
              " 'bare',\n",
              " 'basebal',\n",
              " 'basement',\n",
              " 'basi',\n",
              " 'basic',\n",
              " 'basketbal',\n",
              " 'bat',\n",
              " 'bath',\n",
              " 'bathroom',\n",
              " 'beach',\n",
              " 'bean',\n",
              " 'beast',\n",
              " 'beaten',\n",
              " 'becam',\n",
              " 'bedroom',\n",
              " 'beer',\n",
              " 'began',\n",
              " 'begun',\n",
              " 'behalf',\n",
              " 'behav',\n",
              " 'behavior',\n",
              " 'behaviour',\n",
              " 'belief',\n",
              " 'belov',\n",
              " 'beneath',\n",
              " 'benefit',\n",
              " 'berat',\n",
              " 'berlin',\n",
              " 'besid',\n",
              " 'bet',\n",
              " 'betray',\n",
              " 'beyond',\n",
              " 'bicycl',\n",
              " 'bid',\n",
              " 'biggest',\n",
              " 'bike',\n",
              " 'bit',\n",
              " 'bite',\n",
              " 'bitten',\n",
              " 'bitter',\n",
              " 'blackmail',\n",
              " 'blade',\n",
              " 'blank',\n",
              " 'blast',\n",
              " 'bleed',\n",
              " 'bless',\n",
              " 'blind',\n",
              " 'block',\n",
              " 'blond',\n",
              " 'bloodi',\n",
              " 'blossom',\n",
              " 'blown',\n",
              " 'blue',\n",
              " 'bodyguard',\n",
              " 'bombay',\n",
              " 'bomber',\n",
              " 'bond',\n",
              " 'bone',\n",
              " 'boot',\n",
              " 'booth',\n",
              " 'border',\n",
              " 'bore',\n",
              " 'born',\n",
              " 'borrow',\n",
              " 'boston',\n",
              " 'bottl',\n",
              " 'bottom',\n",
              " 'bought',\n",
              " 'boulder',\n",
              " 'bound',\n",
              " 'bounti',\n",
              " 'bow',\n",
              " 'bowl',\n",
              " 'boxer',\n",
              " 'bracelet',\n",
              " 'brain',\n",
              " 'branch',\n",
              " 'brave',\n",
              " 'brawl',\n",
              " 'breakdown',\n",
              " 'breakfast',\n",
              " 'breath',\n",
              " 'bribe',\n",
              " 'brick',\n",
              " 'bridg',\n",
              " 'brief',\n",
              " 'briefcas',\n",
              " 'briefli',\n",
              " 'bright',\n",
              " 'brilliant',\n",
              " 'britain',\n",
              " 'broadcast',\n",
              " 'broadway',\n",
              " 'broke',\n",
              " 'brown',\n",
              " 'bu',\n",
              " 'bucket',\n",
              " 'bug',\n",
              " 'built',\n",
              " 'bull',\n",
              " 'bullet',\n",
              " 'bulli',\n",
              " 'bump',\n",
              " 'burglar',\n",
              " 'burst',\n",
              " 'bush',\n",
              " 'businessman',\n",
              " 'butcher',\n",
              " 'cab',\n",
              " 'cabin',\n",
              " 'cafe',\n",
              " 'cage',\n",
              " 'cake',\n",
              " 'california',\n",
              " 'calm',\n",
              " 'campaign',\n",
              " 'campu',\n",
              " 'canadian',\n",
              " 'cancel',\n",
              " 'cancer',\n",
              " 'cannon',\n",
              " 'canyon',\n",
              " 'capabl',\n",
              " 'capit',\n",
              " 'captiv',\n",
              " 'card',\n",
              " 'cargo',\n",
              " 'carniv',\n",
              " 'cart',\n",
              " 'cartoon',\n",
              " 'cash',\n",
              " 'casino',\n",
              " 'cast',\n",
              " 'castl',\n",
              " 'cathol',\n",
              " 'cattl',\n",
              " 'cavalri',\n",
              " 'cave',\n",
              " 'cellar',\n",
              " 'cemeteri',\n",
              " 'center',\n",
              " 'centr',\n",
              " 'central',\n",
              " 'centuri',\n",
              " 'ceremoni',\n",
              " 'certain',\n",
              " 'chain',\n",
              " 'chair',\n",
              " 'chamber',\n",
              " 'champion',\n",
              " 'championship',\n",
              " 'channel',\n",
              " 'chao',\n",
              " 'charm',\n",
              " 'cheat',\n",
              " 'cheer',\n",
              " 'chemic',\n",
              " 'chennai',\n",
              " 'chess',\n",
              " 'chest',\n",
              " 'chicago',\n",
              " 'chicken',\n",
              " 'china',\n",
              " 'chines',\n",
              " 'chocol',\n",
              " 'choic',\n",
              " 'chosen',\n",
              " 'christma',\n",
              " 'cia',\n",
              " 'cigarett',\n",
              " 'cinema',\n",
              " 'circl',\n",
              " 'circu',\n",
              " 'circumst',\n",
              " 'citizen',\n",
              " 'civil',\n",
              " 'civilian',\n",
              " 'clan',\n",
              " 'clash',\n",
              " 'classic',\n",
              " 'classmat',\n",
              " 'clean',\n",
              " 'clearli',\n",
              " 'clerk',\n",
              " 'client',\n",
              " 'climax',\n",
              " 'climb',\n",
              " 'clinic',\n",
              " 'clip',\n",
              " 'clock',\n",
              " 'clone',\n",
              " 'closer',\n",
              " 'closet',\n",
              " 'cloud',\n",
              " 'clown',\n",
              " 'clue',\n",
              " 'coach',\n",
              " 'coast',\n",
              " 'coat',\n",
              " 'cocain',\n",
              " 'coffe',\n",
              " 'coffin',\n",
              " 'coin',\n",
              " 'cold',\n",
              " 'collabor',\n",
              " 'colleagu',\n",
              " 'colonel',\n",
              " 'coloni',\n",
              " 'color',\n",
              " 'colorado',\n",
              " 'coma',\n",
              " 'combat',\n",
              " 'combin',\n",
              " 'comedi',\n",
              " 'comfort',\n",
              " 'comic',\n",
              " 'comment',\n",
              " 'commerci',\n",
              " 'commiss',\n",
              " 'commission',\n",
              " 'common',\n",
              " 'communist',\n",
              " 'companion',\n",
              " 'compar',\n",
              " 'compet',\n",
              " 'competit',\n",
              " 'complain',\n",
              " 'complex',\n",
              " 'complic',\n",
              " 'compos',\n",
              " 'compound',\n",
              " 'compromis',\n",
              " 'comput',\n",
              " 'comrad',\n",
              " 'conceal',\n",
              " 'concentr',\n",
              " 'concert',\n",
              " 'conclud',\n",
              " 'conclus',\n",
              " 'condemn',\n",
              " 'conduct',\n",
              " 'confeder',\n",
              " 'confer',\n",
              " 'confin',\n",
              " 'confirm',\n",
              " 'conflict',\n",
              " 'conscious',\n",
              " 'consequ',\n",
              " 'consider',\n",
              " 'consist',\n",
              " 'consol',\n",
              " 'conspir',\n",
              " 'conspiraci',\n",
              " 'constant',\n",
              " 'constantli',\n",
              " 'construct',\n",
              " 'consult',\n",
              " 'consum',\n",
              " 'contempl',\n",
              " 'content',\n",
              " 'contest',\n",
              " 'contract',\n",
              " 'convent',\n",
              " 'convert',\n",
              " 'convict',\n",
              " 'convoy',\n",
              " 'cook',\n",
              " 'cool',\n",
              " 'cope',\n",
              " 'copi',\n",
              " 'corner',\n",
              " 'corp',\n",
              " 'corpor',\n",
              " 'corps',\n",
              " 'correct',\n",
              " 'corrupt',\n",
              " 'cost',\n",
              " 'costum',\n",
              " 'cottag',\n",
              " 'council',\n",
              " 'count',\n",
              " 'countess',\n",
              " 'counti',\n",
              " 'countrysid',\n",
              " 'courag',\n",
              " 'cowboy',\n",
              " 'cowork',\n",
              " 'crack',\n",
              " 'crawl',\n",
              " 'crazi',\n",
              " 'creativ',\n",
              " 'creatur',\n",
              " 'crippl',\n",
              " 'crisi',\n",
              " 'critic',\n",
              " 'crocodil',\n",
              " 'crook',\n",
              " 'crown',\n",
              " 'cruel',\n",
              " 'cruis',\n",
              " 'culmin',\n",
              " 'culprit',\n",
              " 'cult',\n",
              " 'cultur',\n",
              " 'cup',\n",
              " 'cure',\n",
              " 'curiou',\n",
              " 'curli',\n",
              " 'current',\n",
              " 'curs',\n",
              " 'custodi',\n",
              " 'custom',\n",
              " 'da',\n",
              " 'dad',\n",
              " 'daili',\n",
              " 'damag',\n",
              " 'dancer',\n",
              " 'dare',\n",
              " 'dc',\n",
              " 'de',\n",
              " 'deadli',\n",
              " 'deaf',\n",
              " 'dealer',\n",
              " 'debat',\n",
              " 'debt',\n",
              " 'decad',\n",
              " 'decapit',\n",
              " 'deceas',\n",
              " 'deck',\n",
              " 'declin',\n",
              " 'decor',\n",
              " 'dedic',\n",
              " 'deduc',\n",
              " 'deed',\n",
              " 'deep',\n",
              " 'deeper',\n",
              " 'deepli',\n",
              " 'defens',\n",
              " 'delay',\n",
              " 'deliber',\n",
              " 'delight',\n",
              " 'deliveri',\n",
              " 'demon',\n",
              " 'demonstr',\n",
              " 'den',\n",
              " 'departur',\n",
              " 'depict',\n",
              " 'depress',\n",
              " 'deputi',\n",
              " 'descend',\n",
              " 'describ',\n",
              " 'design',\n",
              " 'desir',\n",
              " 'desk',\n",
              " 'despair',\n",
              " 'destin',\n",
              " 'destini',\n",
              " 'destruct',\n",
              " 'detail',\n",
              " 'deton',\n",
              " 'devast',\n",
              " 'devic',\n",
              " 'devil',\n",
              " 'devis',\n",
              " 'devot',\n",
              " 'diari',\n",
              " 'difficult',\n",
              " 'difficulti',\n",
              " 'dig',\n",
              " 'diner',\n",
              " 'dinosaur',\n",
              " 'directli',\n",
              " 'dirti',\n",
              " 'disabl',\n",
              " 'disappoint',\n",
              " 'disapprov',\n",
              " 'disarm',\n",
              " 'disast',\n",
              " 'discoveri',\n",
              " 'diseas',\n",
              " 'disgust',\n",
              " 'dislik',\n",
              " 'dismay',\n",
              " 'dismiss',\n",
              " 'dispatch',\n",
              " 'display',\n",
              " 'dispos',\n",
              " 'disrupt',\n",
              " 'distanc',\n",
              " 'distant',\n",
              " 'distraught',\n",
              " 'distress',\n",
              " 'district',\n",
              " 'disturb',\n",
              " 'dive',\n",
              " 'divid',\n",
              " 'divis',\n",
              " 'doc',\n",
              " 'dock',\n",
              " 'document',\n",
              " 'documentari',\n",
              " 'dodg',\n",
              " 'dollar',\n",
              " 'donat',\n",
              " 'doom',\n",
              " 'doubl',\n",
              " 'doubt',\n",
              " 'downstair',\n",
              " 'dragon',\n",
              " 'drain',\n",
              " 'drama',\n",
              " 'dramat',\n",
              " 'drawn',\n",
              " 'drill',\n",
              " 'driven',\n",
              " 'drown',\n",
              " 'drum',\n",
              " 'drunken',\n",
              " 'duck',\n",
              " 'duel',\n",
              " 'dump',\n",
              " 'duo',\n",
              " 'dust',\n",
              " 'dutch',\n",
              " 'duti',\n",
              " 'dynamit',\n",
              " 'eager',\n",
              " 'eagl',\n",
              " 'ear',\n",
              " 'earthquak',\n",
              " 'easi',\n",
              " 'easili',\n",
              " 'east',\n",
              " 'eaten',\n",
              " 'eccentr',\n",
              " 'edg',\n",
              " 'editor',\n",
              " 'educ',\n",
              " 'egg',\n",
              " 'egypt',\n",
              " 'egyptian',\n",
              " 'eight',\n",
              " 'either',\n",
              " 'el',\n",
              " 'elabor',\n",
              " 'elder',\n",
              " 'elderli',\n",
              " 'eldest',\n",
              " 'elect',\n",
              " 'electr',\n",
              " 'electrocut',\n",
              " 'electron',\n",
              " 'element',\n",
              " 'eleph',\n",
              " 'elev',\n",
              " 'elimin',\n",
              " 'elop',\n",
              " 'elsewher',\n",
              " 'embark',\n",
              " 'embarrass',\n",
              " 'embrac',\n",
              " 'emperor',\n",
              " 'empir',\n",
              " 'employe',\n",
              " 'en',\n",
              " 'enabl',\n",
              " 'enchant',\n",
              " 'endur',\n",
              " 'energi',\n",
              " 'enforc',\n",
              " 'engin',\n",
              " 'england',\n",
              " 'english',\n",
              " 'enlist',\n",
              " 'enorm',\n",
              " 'enrol',\n",
              " 'ensur',\n",
              " 'entertain',\n",
              " 'enthusiast',\n",
              " 'entranc',\n",
              " 'entri',\n",
              " 'epilogu',\n",
              " 'episod',\n",
              " 'equal',\n",
              " 'equip',\n",
              " 'erupt',\n",
              " 'escal',\n",
              " 'escort',\n",
              " 'especi',\n",
              " 'establish',\n",
              " 'estat',\n",
              " 'estrang',\n",
              " 'europ',\n",
              " 'evacu',\n",
              " 'evad',\n",
              " 'everybodi',\n",
              " 'everywher',\n",
              " 'evict',\n",
              " 'exact',\n",
              " 'exactli',\n",
              " 'exam',\n",
              " 'examin',\n",
              " 'exampl',\n",
              " 'excit',\n",
              " 'excus',\n",
              " 'exhaust',\n",
              " 'exhibit',\n",
              " 'exil',\n",
              " 'exist',\n",
              " 'exit',\n",
              " 'expedit',\n",
              " 'expens',\n",
              " 'experienc',\n",
              " 'expert',\n",
              " 'explan',\n",
              " 'explod',\n",
              " 'exploit',\n",
              " 'explor',\n",
              " 'extend',\n",
              " 'extra',\n",
              " 'extract',\n",
              " 'extrem',\n",
              " 'exwif',\n",
              " 'facil',\n",
              " 'factori',\n",
              " 'fade',\n",
              " 'failur',\n",
              " 'faint',\n",
              " 'fair',\n",
              " 'fairi',\n",
              " 'fals',\n",
              " 'fame',\n",
              " 'fang',\n",
              " 'fantasi',\n",
              " 'farewel',\n",
              " 'farm',\n",
              " 'farmer',\n",
              " 'fascin',\n",
              " 'fashion',\n",
              " 'fast',\n",
              " 'fat',\n",
              " 'fatal',\n",
              " 'fault',\n",
              " 'favor',\n",
              " 'favorit',\n",
              " 'fbi',\n",
              " 'fed',\n",
              " 'feder',\n",
              " 'feed',\n",
              " 'feet',\n",
              " 'fell',\n",
              " 'felt',\n",
              " 'fenc',\n",
              " 'fend',\n",
              " 'ferri',\n",
              " 'festiv',\n",
              " 'fianc',\n",
              " 'fiance',\n",
              " 'fiancé',\n",
              " 'fiction',\n",
              " 'fierc',\n",
              " 'fighter',\n",
              " 'file',\n",
              " 'filmmak',\n",
              " 'financ',\n",
              " 'financi',\n",
              " 'fine',\n",
              " 'finger',\n",
              " 'firm',\n",
              " 'fish',\n",
              " 'fit',\n",
              " 'fix',\n",
              " 'flag',\n",
              " 'flame',\n",
              " 'flash',\n",
              " 'flashback',\n",
              " 'flat',\n",
              " 'fleet',\n",
              " 'flesh',\n",
              " 'flight',\n",
              " 'flirt',\n",
              " 'float',\n",
              " 'flood',\n",
              " 'focu',\n",
              " 'focus',\n",
              " 'foil',\n",
              " 'fool',\n",
              " 'foot',\n",
              " 'footag',\n",
              " 'footbal',\n",
              " 'forbid',\n",
              " 'forbidden',\n",
              " 'foreign',\n",
              " 'forev',\n",
              " 'forg',\n",
              " 'forget',\n",
              " 'forgiv',\n",
              " 'forgotten',\n",
              " 'formula',\n",
              " 'fort',\n",
              " 'forth',\n",
              " 'forward',\n",
              " 'fourth',\n",
              " 'frame',\n",
              " 'frankenstein',\n",
              " 'frantic',\n",
              " 'fratern',\n",
              " 'fraud',\n",
              " 'freed',\n",
              " 'freedom',\n",
              " 'freez',\n",
              " 'frequent',\n",
              " 'fresh',\n",
              " 'friendli',\n",
              " 'frighten',\n",
              " 'frozen',\n",
              " 'fu',\n",
              " 'fuel',\n",
              " 'fugit',\n",
              " 'fulfil',\n",
              " 'fulli',\n",
              " 'fun',\n",
              " 'function',\n",
              " 'fund',\n",
              " 'funer',\n",
              " 'furiou',\n",
              " 'ga',\n",
              " 'gag',\n",
              " 'gambl',\n",
              " 'gambler',\n",
              " 'gangster',\n",
              " 'garag',\n",
              " 'gather',\n",
              " 'german',\n",
              " 'germani',\n",
              " 'getaway',\n",
              " 'ghost',\n",
              " 'giant',\n",
              " 'glass',\n",
              " 'glimps',\n",
              " 'glow',\n",
              " 'goal',\n",
              " 'gold',\n",
              " 'golden',\n",
              " 'golf',\n",
              " 'goodby',\n",
              " 'goon',\n",
              " 'gotten',\n",
              " 'governor',\n",
              " 'grade',\n",
              " 'gradual',\n",
              " 'graduat',\n",
              " 'grand',\n",
              " 'granddaught',\n",
              " 'grandfath',\n",
              " 'grandmoth',\n",
              " 'grandson',\n",
              " 'grate',\n",
              " 'grave',\n",
              " 'greatest',\n",
              " 'green',\n",
              " 'greet',\n",
              " 'grew',\n",
              " 'grief',\n",
              " 'groceri',\n",
              " 'groom',\n",
              " 'grown',\n",
              " 'guardian',\n",
              " 'guest',\n",
              " 'guid',\n",
              " 'guilt',\n",
              " 'guilti',\n",
              " 'guitar',\n",
              " 'gunfight',\n",
              " 'gunpoint',\n",
              " 'gunshot',\n",
              " 'gym',\n",
              " 'habit',\n",
              " 'hack',\n",
              " 'hair',\n",
              " 'halloween',\n",
              " 'hammer',\n",
              " 'handl',\n",
              " 'handsom',\n",
              " 'harass',\n",
              " 'harbor',\n",
              " 'hardi',\n",
              " 'harm',\n",
              " 'harvest',\n",
              " 'hat',\n",
              " 'hatch',\n",
              " 'hate',\n",
              " 'hatr',\n",
              " 'haunt',\n",
              " 'havoc',\n",
              " 'hawk',\n",
              " 'headquart',\n",
              " 'heal',\n",
              " 'health',\n",
              " 'heard',\n",
              " 'heartbroken',\n",
              " 'heat',\n",
              " 'heaven',\n",
              " 'heavi',\n",
              " 'heavili',\n",
              " 'heir',\n",
              " 'heist',\n",
              " 'helicopt',\n",
              " 'hell',\n",
              " 'helpless',\n",
              " 'henc',\n",
              " 'henchman',\n",
              " 'henchmen',\n",
              " 'herd',\n",
              " 'heroin',\n",
              " 'hesit',\n",
              " 'hideout',\n",
              " 'higher',\n",
              " 'highli',\n",
              " 'highway',\n",
              " 'hijack',\n",
              " 'hint',\n",
              " 'histor',\n",
              " 'histori',\n",
              " 'hitler',\n",
              " 'hockey',\n",
              " 'hole',\n",
              " 'holi',\n",
              " 'holiday',\n",
              " 'hollywood',\n",
              " 'homeless',\n",
              " 'hometown',\n",
              " 'honest',\n",
              " 'honeymoon',\n",
              " 'hong',\n",
              " 'hood',\n",
              " 'hook',\n",
              " 'horn',\n",
              " 'horribl',\n",
              " 'horrifi',\n",
              " 'horror',\n",
              " 'hose',\n",
              " 'host',\n",
              " 'hostag',\n",
              " 'hostil',\n",
              " 'hot',\n",
              " 'household',\n",
              " 'housekeep',\n",
              " 'hug',\n",
              " 'humili',\n",
              " 'humor',\n",
              " 'hundr',\n",
              " 'hurt',\n",
              " 'hut',\n",
              " 'hypnot',\n",
              " 'hyster',\n",
              " 'ice',\n",
              " 'idol',\n",
              " 'ii',\n",
              " 'illeg',\n",
              " 'im',\n",
              " 'imag',\n",
              " 'imagin',\n",
              " 'immigr',\n",
              " 'immin',\n",
              " 'impal',\n",
              " 'impend',\n",
              " 'imperi',\n",
              " 'imperson',\n",
              " 'impli',\n",
              " 'implic',\n",
              " 'imposs',\n",
              " 'imprison',\n",
              " 'improv',\n",
              " 'inadvert',\n",
              " 'incom',\n",
              " 'increas',\n",
              " 'increasingli',\n",
              " 'inde',\n",
              " 'independ',\n",
              " 'india',\n",
              " 'indian',\n",
              " 'indic',\n",
              " 'individu',\n",
              " 'industri',\n",
              " 'inevit',\n",
              " 'infant',\n",
              " 'infatu',\n",
              " 'infect',\n",
              " 'infiltr',\n",
              " 'influenc',\n",
              " 'infuri',\n",
              " 'inhabit',\n",
              " 'inherit',\n",
              " 'inject',\n",
              " 'injuri',\n",
              " 'inmat',\n",
              " 'inn',\n",
              " 'inner',\n",
              " 'insan',\n",
              " 'inspect',\n",
              " 'inspir',\n",
              " 'instal',\n",
              " 'instantli',\n",
              " 'institut',\n",
              " 'instruct',\n",
              " 'instructor',\n",
              " 'instrument',\n",
              " 'insult',\n",
              " 'insur',\n",
              " 'intellig',\n",
              " 'intens',\n",
              " 'interact',\n",
              " 'intercept',\n",
              " 'interfer',\n",
              " 'intern',\n",
              " 'interrog',\n",
              " 'interven',\n",
              " 'intrigu',\n",
              " 'invad',\n",
              " 'invas',\n",
              " 'invent',\n",
              " 'invest',\n",
              " 'invis',\n",
              " 'ireland',\n",
              " 'irish',\n",
              " 'iron',\n",
              " 'irrit',\n",
              " 'isol',\n",
              " 'issu',\n",
              " 'itali',\n",
              " 'italian',\n",
              " 'item',\n",
              " 'jacket',\n",
              " 'janitor',\n",
              " 'japan',\n",
              " 'japanes',\n",
              " 'jar',\n",
              " 'jealou',\n",
              " 'jealousi',\n",
              " 'jet',\n",
              " 'jew',\n",
              " 'jewish',\n",
              " 'johnson',\n",
              " 'joke',\n",
              " 'jone',\n",
              " 'journalist',\n",
              " 'jr',\n",
              " 'judg',\n",
              " 'jungl',\n",
              " 'junior',\n",
              " 'juri',\n",
              " 'justic',\n",
              " 'kept',\n",
              " 'khan',\n",
              " 'kick',\n",
              " 'kidnapp',\n",
              " 'kind',\n",
              " 'kingdom',\n",
              " 'kitchen',\n",
              " 'knew',\n",
              " 'knife',\n",
              " 'knight',\n",
              " 'knowledg',\n",
              " 'kong',\n",
              " 'korean',\n",
              " 'kumar',\n",
              " 'kung',\n",
              " 'lab',\n",
              " 'label',\n",
              " 'labor',\n",
              " 'laboratori',\n",
              " 'lack',\n",
              " 'ladder',\n",
              " 'laid',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-uRZK3EPSyl"
      },
      "source": [
        "Η συνάρτηση [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) όπως καλείται εδώ **δεν είναι βελτιστοποιημένη**. Οι επιλογές των μεθόδων και παραμέτρων της μπορεί να έχουν **δραματική επίδραση στην ποιότητα των συστάσεων** και είναι διαφορετικές για κάθε dataset. Επίσης, οι επιλογές αυτές έχουν πολύ μεγάλη επίδραση και στη **διαστατικότητα και όγκο των δεδομένων**. Η διαστατικότητα των δεδομένων με τη σειρά της θα έχει πολύ μεγάλη επίδραση στους **χρόνους εκπαίδευσης**, ιδιαίτερα στη δεύτερη εφαρμογή της άσκησης. Ανατρέξτε στα notebooks του εργαστηρίου και στο [FAQ](https://docs.google.com/document/d/1hou1gWXQuHAB7J2aV44xm_CtAWJ63q6Cu1V6OwyL_n0/edit?usp=sharing) των ασκήσεων.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LsmvSyVykTU"
      },
      "source": [
        "## Υλοποίηση του συστήματος συστάσεων\n",
        "\n",
        "Το σύστημα συστάσεων που θα παραδώσετε θα είναι μια συνάρτηση `content_recommender` με δύο ορίσματα `target_movie` και `max_recommendations`. Στην `target_movie` περνάμε το ID μιας ταινίας-στόχου για την οποία μας ενδιαφέρει να βρούμε παρόμοιες ως προς το περιεχόμενο (τη σύνοψη) ταινίες, `max_recommendations` στο πλήθος.\n",
        "Υλοποιήστε τη συνάρτηση ως εξής: \n",
        "- για την ταινία-στόχο, από το `corpus_tf_idf` υπολογίστε την [ομοιότητα συνημιτόνου](https://en.wikipedia.org/wiki/Cosine_similarity) της με όλες τις ταινίες της συλλογής σας\n",
        "- με βάση την ομοιότητα συνημιτόνου που υπολογίσατε, δημιουργήστε ταξινομημένο πίνακα από το μεγαλύτερο στο μικρότερο, με τα indices (`ID`) των ταινιών. Παράδειγμα: αν η ταινία με index 1 έχει ομοιότητα συνημιτόνου με 3 ταινίες \\[0.2 1 0.6\\] (έχει ομοιότητα 1 με τον εαύτό της) ο ταξινομημένος αυτός πίνακας indices θα είναι \\[1 2 0\\].\n",
        "- Για την ταινία-στόχο εκτυπώστε: id, τίτλο, σύνοψη, κατηγορίες (categories)\n",
        "- Για τις `max_recommendations` ταινίες (πλην της ίδιας της ταινίας-στόχου που έχει cosine similarity 1 με τον εαυτό της) με τη μεγαλύτερη ομοιότητα συνημιτόνου (σε φθίνουσα σειρά), τυπώστε σειρά σύστασης (1 πιο κοντινή, 2 η δεύτερη πιο κοντινή κλπ), id, τίτλο, σύνοψη, κατηγορίες (categories)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVeo6iscH6mU"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "\r\n",
        "def content_recommender(target_movie, max_recommendations):\r\n",
        "  target_corpus = corpus_tf_idf[target_movie]\r\n",
        "  N = corpus_tf_idf.shape[0]\r\n",
        "  cos_sims = np.zeros(N)\r\n",
        "  for i in range(N):\r\n",
        "    cos_sims[i] = cosine_similarity(target_corpus, corpus_tf_idf[i])\r\n",
        "  cos_sims_ind = np.argsort(cos_sims)[::-1]\r\n",
        "  print(\"Target movie:\\nId: {0}, Title: {1}, Summary: {2}, Categories: {3}\".format(\r\n",
        "        target_movie, titles[target_movie][0], summaries[target_movie][0], categories[target_movie][0]))\r\n",
        "  print(\"Recommendations:\")\r\n",
        "  i = 1\r\n",
        "  for x in cos_sims_ind[1:max_recommendations+1]:\r\n",
        "    print(\"{0}, Id: {1}, Title: {2}, Summary: {3}, Categories: {4}\".format(\r\n",
        "          i, x, titles[x][0], summaries[x][0], categories[x][0]))\r\n",
        "    i += 1\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRBC-tNcMh6k",
        "outputId": "231e3328-5de1-440f-abfd-fc854f97ed9b"
      },
      "source": [
        "content_recommender(120, 10)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target movie:\n",
            "Id: 120, Title: Babylon 5: The Gathering, Summary:  In the earth year 2257, a multitude of humans and non-humans gather deep in neutral space at a new station, Babylon 5, which has recently become operational. Babylon 5 was built as a neutral venue for discussing and resolving issues between the five major spacefaring races of the galaxy, the humans, Narn, Centauri, Minbari and Vorlons. Commander Jeffrey Sinclair is in overall charge of the station. As the crew awaits the arrival of the fourth and final alien ambassador, Ambassador Kosh Naranek from the Vorlon Empire, a transport ship arrives from Earth, bearing Lyta Alexander, a human telepath who joins the station crew, and Del Varner, a civilian. Ambassador Kosh arrives two days ahead of schedule, and is on board the station less than a minute when he suddenly falls ill, apparently from poisoning. Babylon 5's chief medical officer, Dr. Kyle, conducts a medical investigation and seeks to prevent Kosh's death, while Security Chief Michael Garibaldi conducts a security investigation. Worried that, if Kosh dies, the Vorlons will attack and destroy the station, Dr. Kyle and Lt. Cmdr. Takashima persuade Lyta to perform an unauthorized mind scan on the unconscious Kosh. As she conducts the scan, Lyta sees Commander Sinclair poisoning the Ambassador. Lyta accuses Sinclair of attempted murder. A meeting of Babylon 5's Council, made up of delegates from all five races, resolves to extradite the Commander to the Vorlon homeworld for trial. Sinclair is told that he will be deported in twelve hours. Garibaldi comes to suspect Del Varner might have been involved in Kosh's poisoning. When he enters Varner's quarters, however, he discovers Varner dead in a fish tank. Lyta enters the medical lab, where she begins adjusting some of the settings that are keeping Kosh alive; when Dr. Kyle realizes what she's doing, he tries to stop her, and she attacks him. At that moment, the real Lyta Alexander enters the room; her double escapes. Upon further investigation in Varner's quarters, Garibaldi learns that Varner had been smuggling illegal items between systems, and that he most recently had gone to the Antares sector to acquire a changeling net; a device that can make an individual appear to look like somebody else. The crew realizes that Kosh had not been poisoned by Sinclair when he arrived at the station, but rather that he had been poisoned by someone who was using the changeling net to imitate Sinclair. Since the use of such a device would put out a lot of energy, Takashima uses her scanners to pinpoint an area of the station with a high concentration of unidentified energy use. Sinclair and Garibaldi head for that part of the station, just as a Vorlon squadron arrives in the vicinity of the station to pick up Sinclair for his voyage to the Vorlon homeworld. Sinclair and Garibaldi confront the mysterious assailant. Garibaldi is injured in the firefight and Sinclair faces the assassin himself. The changeling net is disabled, revealing the assailant to be a Minbari assassin. The assassin is a member of the Minbari warrior caste, and wanted to discredit Sinclair as retribution for Sinclair's role in the Earth-Minbari war ten years earlier. Sinclair asks the assassin why he did it; the assassin replies, \"There is a hole in your mind.\" Sinclair, being informed that the assassin has triggered an explosive charge, manages to get away just before an explosion rips a hole in the station's hull, throwing the station off its axis, and beginning to tear the station apart from the inside. Takashima uses the station stabilizers to reestablish the station's axis. The Vorlon delegation, now satisfied that Sinclair is innocent, drops all charges against him. In the station's garden, Sinclair reveals to Delenn what the Minbari assassin had said about the \"hole\" in Sinclair's mind. Delenn claims that it is just an old Minbari insult. Sinclair, however, tells her that he had fought in the climactic battle of the Earth-Minbari war, and that there is a twenty-four hour period in the climactic battle, just before the Minbari surrendered, that he can't account for. Takashima declares Babylon 5 open for business., Categories: \"Science Fiction\",  \"Adventure\",  \"Television movie\",  \"Mystery\",  \"Drama\",  \"Whodunit\"\n",
            "Recommendations:\n",
            "1, Id: 1335, Title: Hissatsu: Sure Death, Summary: A group of common merchants in Edo  in the mid-1800s is really a band of assassins available for hire. Each assassin has his or her own unique killing style. But one assassin is killing other assassins, and the survivors must find out who is doing it before they are all killed. The rogue killer is referred to as \"Copper\" in the film, a reference to the assassin's gruesome calling card, which is a copper coin left in the mouth of each victim. The ultimate realization that \"Copper\" commands an army of assassins simply means that the climax involves an orgy of assassinations., Categories: \"Drama\"\n",
            "2, Id: 133, Title: Break, Summary: A terminally ill crime boss, known only as The Man, hires a hit man named Frank to carry out his own assassination as well as the assassination of The Woman he loves. When The Woman turns out to be Frank's long lost lover, he turns against The Man becoming the target himself., Categories: \"Crime Fiction\",  \"Crime Thriller\",  \"Action\",  \"Gangster Film\",  \"Drama\"\n",
            "3, Id: 4771, Title: Yakuza Demon, Summary:  Seiji  grew up as an orphan who was raised by Boss Muto, whom Seiji considers his father. The Muto family is composed only of Boss Muto, Seiji and Yoshi. However they are part of the Date syndicate. A rival syndicate, the Tendo, attack the Date family, which leads to a meeting in which Boss Muto was required to pay money to help finance the impending war. However Muto does not have enough so offers an assassination instead. To stop his boss from serving a sentence for murder, Seiji informs the police where Muto hides his gun. Boss Muto gets arrested and sentenced to serve 2 years in prison. Seiji and Yoshi rob the financier of the Tendo group and buy weapons from a Taiwanese dealer. The two then attempt to murder the head boss of the 15,000 member strong Tendo group. They nearly succeed as the boss is in critical condition. The act has compromised the future of the Date family. The Tendo family respond quickly with a wave of brutal murders. Seiji and Yoshi return to the Taiwanese dealer and tell him the two plan to attack the number two man of the Tendo family. Boss Muto is expelled from the Date clan, while in prison, because of Seiji's actions. Yoshi gets killed by an assassin squad under the order of the Tendo family, while visiting his girlfriend. After hearing of his bosses expulsion Seiji returns to the Date headquarters and shoots the first person who yells at him. A member of the Date clan explains boss Muto was expelled because he went to prison as when he was supposed to commit an assassination, so they think Boss Muto himself in to police. Seiji readily explains it was him who told on the boss. Seiji carries out the assassination of the number two boss and is shot shortly after. Seiji then goes to the bar of Boss Muto's wife, who takes him to a doctor. Soon after, Boss Date is assassinated. The Taiwanese arms dealer shows up at Boss Muto's wife's house with weapons. Before he leaves he tells her that Seiji must love her because of his actions. Boss Muto is murdered in prison. Seiji then retaliates by killing boss Muto's killer on the way to his trial. Seiji and boss Muto's wife plan to escape to the Philippines. As their boat begins to arrive, Boss Muto's wife throws Seiji's gun into the water with hopes of a peaceful future. As they turn to get their luggage, they notice a policeman, who had provided information to Seiji, hanging from a crane nearby. The two become aware they are cornered at the end of the dock and their boat turns around when it sees trouble. The assassin squad closes in on them and open fire as Seiji blocks all the shots from hitting Muto's wife. As Seiji lies dead, the leader of the assassin squad tells him that he's \"way too cool\" and walks off without harming boss Muto's wife., Categories: \"Thriller\",  \"Crime Fiction\",  \"Japanese Movies\",  \"World cinema\",  \"Gangster Film\",  \"Action/Adventure\",  \"Crime Thriller\",  \"Action\"\n",
            "4, Id: 995, Title: I as in Icarus, Summary: The film's plot is based on the Kennedy assassination and subsequent investigation. The film begins with the assassination of President Marc Jarry, who is about to be inaugurated for a second six-year term of office. Henri Volney, state attorney and member of the commission charged with investigating the assassination  refuses to agree to the commission's final findings. The film portrays the initial controversy about this, as well as Volney and his staff's reopening of the investigation., Categories: \"Thriller\"\n",
            "5, Id: 2525, Title: Trancers 6, Summary: In a return to the groundbreaking original film's premise, Jack Deth is back - traveling back in time and into the body of his own daughter, Josephine , on a mission to save her life and save the world from the most lethal Trancers yet. Jack/Jo must adapt and survive being a girl while avoiding many assassination attempts by more powerful and dangerous zombie-like Trancers than he's ever faced before., Categories: \"Thriller\",  \"Action/Adventure\",  \"Science Fiction\",  \"Action\",  \"Action Thrillers\"\n",
            "6, Id: 4387, Title: Cat And Mouse, Summary: Zhan Zhao  is a court officer who learns of a plot to assassinate Judge Bao . While on holiday he meets a young man named Bai  who turns out to be a woman. Zhan Zhao tries to recruit Bai to help him stop the assassination of Judge Bao., Categories: \"Romantic comedy\",  \"World cinema\",  \"Action/Adventure\",  \"Drama\",  \"Comedy\",  \"Romantic drama\",  \"Romance Film\",  \"Chinese Movies\"\n",
            "7, Id: 3484, Title: Traffic, Summary: {{Plot|dateMexico storylineWakefield storylineAyala/DEA storyline A third story is set in San Diego, where an undercover DEA investigation led by Montel Gordon  and Ray Castro  leads to the arrest of Eduardo Ruiz , a high-stakes dealer posing as a fisherman. Ruiz decides to take the dangerous road to immunity by giving up his boss: drug lord Carlos Ayala , the biggest distributor for the Obregón brothers in the United States. Ayala is indicted by a tough prosecutor, hand-selected by Robert to send a message to the Mexican drug organizations. As the trial against Carlos Ayala begins, his pregnant wife Helena  learns of her husband's true profession. Facing the prospect of life imprisonment for her husband and death threats against her only child, Helena decides to hire Flores to assassinate Eduardo Ruiz; she knows killing Ruiz will effectively end the trial nolle prosequi. Flores plants a car bomb on a DEA car in an assassination attempt against Ruiz. Shortly after planting the bomb, Flores is assassinated by a sniper in retaliation for his co-operation with General Salazar; the car bomb kills Castro, but Gordon and Ruiz survive. Helena, knowing Ruiz is soon scheduled to testify, makes a deal with Juan Obregón , lord of the drug cartel, who forgives the debt of the Ayala family and has Ruiz poisoned. Ayala is released, much to the dissatisfaction of Gordon, who is still angry over the death of his partner. Soon after the release, Gordon bursts into the Ayala home and surreptitiously plants a listening bug under his desk and leaves., Categories: \"Thriller\",  \"Crime Fiction\",  \"Gangster Film\",  \"Drama\",  \"Crime Thriller\",  \"Political drama\"\n",
            "8, Id: 2475, Title: Executive Action, Summary: The movie starts with a voice over stating that in an interview, President Lyndon Johnson was asked about the Kennedy Assassination and the Warren Commission report: he said he doubted seriously the findings of the Commission. The narration ends with the mention that the segment did not run on television and was cut from a program about Johnson.  The movie opens in June 1963 at a gathering of shadowy industrial, political and former US intelligence figures who are giving vent to their growing dissatisfaction with the Kennedy administration. The scene takes place in the plush surroundings of the lead conspirator, Robert Foster , presumably a Texas oil baron. He and the others are trying to convince Ferguson , a white-suited and mustachioed figure — a powerful oil magnate — to back their plans for an assassination of Kennedy. He remains unconvinced, saying \"I don't like such schemes. They're only tolerable when necessary, and only permissible when they work.\" James Farrington , a black ops specialist, is also among the group. The film then cuts to an unknown location in the desert where a shooting team is doing target practice at a moving object. One of the shooters says that they can only guarantee the operation's success by slowing down the target to 15&nbsp;mph. The film intercuts between conversations among the lead conspirators, Farrington and Foster, and preparations for the assassination. The approval of the man in the white suit is crucial to the conspirators, although Farrington proceeds to organize two shooting teams in anticipation that he will change his mind. We then see sequences of the man in the white suit watching contemporary newsreel and becoming clearly concerned at Kennedy's increasingly 'liberal' direction: action on civil rights, Nuclear Test Ban Treaty, nuclear disarmament. The deciding moment comes when he's watching an anti-Kennedy news report on the deteriorating situation in South Vietnam. It is followed by Kennedy's 'suicidal' October 1963 decision  to withdraw all US advisors from Vietnam by the end of 1965, effectively ending America's direct involvement in the Vietnam War. He picks up the phone to tell Foster he now fully supports their project. While the motives of the man in the white suit are clear, the film attempts to cast light on the murky paranoid fears of the conspirators through dialogues between Foster and Farrington. They are primarily concerned about the future of America and the security of ruling class white people around the world. Foster forecasts the population of the third world in 2000 at 7 billion, 'Most of them yellow, brown or black. All hungry and all determined to love; they'll swarm out of their breeding grounds into Europe and America'. He sees Vietnam as an opportunity to control the developing world and reduce its population to 550 million: 'I've seen the data,' says Foster, adding that they can then apply the same 'birth-control' methods to unwanted groups in the US: poor whites, blacks and Latinos. At the end of the film a photo collage is shown of 18 witnesses: all but two of whom died from unnatural causes within three years of the assassination. A voice-over says that an actuary of the British newspaper The Sunday Times calculated the probability that all these people who witnessed the assassination would die within that period of time to be 1000 trillion to one.The number given in The Sunday Times article on February 26, 1967 was in fact 100,000 trillion to one. In response to a request by the House Select Committee on Assassinations in 1978 for a copy of the actuarial study, the legal manager for the Times replied that the article was \"based on a careless journalistic mistake and should not have been published. This was realized by The Sunday Times editorial staff after the first edition — the one which goes to the United States and which I believe you have — had gone out, and later editions were amended … We asked [the actuary] the wrong question … what were the odds against fifteen named people out of the population of the United States dying within a short period of time … [instead of] the odds against fifteen of those included in the Warren Commission index dying within a given period,\" which they said would have been \"much lower.\" HSCA Hearings, vol. 4, p. 463–465. Robert M. Musen, vice president and senior actuary at Metropolitan Life Insurance Company, estimated that the odds of 15 people out of 2,479 in the Warren Commission index dying within a three-year period, assuming a median age of 40, would be 98.16 percent, or one out of 1.2. Assuming a median age of 35, the number would be 57.09 percent, or one out of 1.75. Vincent Bugliosi, Reclaiming History , p. 1013–14.  The film postulates that Lee Harvey Oswald is being steered to become the conspiracy's 'patsy'. The conspirators use a double of Oswald to shadow him in the weeks leading up to the assassination to leave behind a trail that the authorities can easily follow and link Oswald to the crime. The film makes no explicit link to US government agencies and the conspiracy, although the professionalism of Farrington's shooting team clearly indicates they have worked for the CIA on special assignments. The film implies that most of the law enforcement and government agencies were not involved, but just grossly inept: no special measures were taken for the president's safety in Dallas; there is no communication between the FBI, CIA and Secret Service on possible security risks; even the head of the Secret Service stays in Washington during the visit. This explanation helps understand why the authorities were so keen to pin the blame on Oswald, the rogue assassin, who is 'served up' by the conspirators to the authorities as an easy escape from any accusations of their own negligence. The post-assassination conspiracy is also covered in the film. Farrington tells the head of the shooting teams, who at this point don't know who their target is, that after this job he and his men will never have to work again. All the assassins are black ops professionals trained never to talk about operations they are involved in. Each one is offered $25,000 per year for the next five years provided the operation's cover isn't blown. If the cover remains intact in five years time  'every man jack of them'  will receive a further $100,000 into their Swiss bank accounts. The head of the shooting teams then tells Farrington: 'You just told me who we're going to hit.', Categories: \"Crime Fiction\",  \"Action/Adventure\",  \"Drama\",  \"Political thriller\",  \"Action\",  \"Political drama\"\n",
            "9, Id: 3969, Title: Who Killed Dr Bogle and Mrs Chandler?, Summary: When the half-naked bodies of brilliant physicist, Dr Gilbert Bogle, and his lover, Mrs Margaret Chandler, were found in bizarre circumstances on a Sydney riverbank in 1963, it set into play an unprecedented forensic investigation. Autopsies offered little clue as to how the couple died, only that there were signs of a rapidly acting poison. Despite assistance from the FBI and Scotland Yard, the poison was never identified. At the end of a long and controversial coronial inquest, no cause of death, killer or motive could be identified. In the ensuing years, scores of tabloid theories have been put forward, from LSD to Cold War assassinations. But in the minds of many, including the police, Margaret Chandler’s husband, Geoffrey, was the likely culprit. Four decades later, this explosive documentary reveals startling new scientific evidence - evidence so powerful the police gave filmmaker Peter Butt unprecedented access to their forensic records., Categories: \"Indie\",  \"Documentary\"\n",
            "10, Id: 2658, Title: Cool Dimension: Innocent Assassin, Summary: Kidnapped in childhood and raised as an assassin, Shiori has spent her entire life following orders of her mysterious masters. But when the childhood memory suddenly resurfaces, the programming in her head is short circuited, her entire world is upside down! Now, before the masters realizes she's slipped their leash, Shiori has to escape. But to do so means she must kill again... even the ones who are close to her., Categories: \"Action/Adventure\",  \"Japanese Movies\",  \"Action\",  \"Drama\",  \"World cinema\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IvHkTUHyu78"
      },
      "source": [
        "## Βελτιστοποίηση\n",
        "\n",
        "Αφού υλοποιήσετε τη συνάρτηση `content_recommender` χρησιμοποιήστε τη για να βελτιστοποιήσετε την `TfidfVectorizer`. Συγκεκριμένα, αρχικά μπορείτε να δείτε τι επιστρέφει το σύστημα για τυχαίες ταινίες-στόχους και για ένα μικρό `max_recommendations` (2 ή 3). Αν σε κάποιες ταινίες το σύστημα μοιάζει να επιστρέφει σημασιολογικά κοντινές ταινίες σημειώστε το `ID` τους. Δοκιμάστε στη συνέχεια να βελτιστοποιήσετε την `TfidfVectorizer` για τα συγκεκριμένα `ID` ώστε να επιστρέφονται σημασιολογικά κοντινές ταινίες για μεγαλύτερο αριθμό `max_recommendations`. Παράλληλα, όσο βελτιστοποιείτε την `TfidfVectorizer`, θα πρέπει να λαμβάνετε καλές συστάσεις για μεγαλύτερο αριθμό τυχαίων ταινιών. Μπορείτε επίσης να βελτιστοποιήσετε τη συνάρτηση παρατηρώντας πολλά φαινόμενα που το σύστημα εκλαμβάνει ως ομοιότητα περιεχομένου ενώ επί της ουσίας δεν είναι επιθυμητό να συνυπολογίζονται (δείτε σχετικά το [FAQ](https://docs.google.com/document/d/1hou1gWXQuHAB7J2aV44xm_CtAWJ63q6Cu1V6OwyL_n0/edit?usp=sharing)). Ταυτόχρονα, μια άλλη κατεύθυνση της βελτιστοποίησης είναι να χρησιμοποιείτε τις παραμέτρους του `TfidfVectorizer` έτσι ώστε να μειώνονται οι διαστάσεις του Vector Space Model μέχρι το σημείο που θα αρχίσει να εμφανίζονται επιπτώσεις στην ποιότητα των συστάσεων. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPVK7Z5c1p5F"
      },
      "source": [
        "## Επεξήγηση επιλογών και ποιοτική ερμηνεία\n",
        "\n",
        "Σε markdown περιγράψτε πώς προχωρήσατε στις επιλογές σας για τη βελτιστοποίηση της `TfidfVectorizer`. Επίσης σε markdown δώστε 10 παραδείγματα (IDs) από τη συλλογή σας που επιστρέφουν καλά αποτελέσματα μέχρι `max_recommendations` (5 και παραπάνω) και σημειώστε συνοπτικά ποια είναι η θεματική που ενώνει τις ταινίες.\n",
        "\n",
        "Δείτε [εδώ](https://pastebin.com/raw/ZEvg5t3z) ένα παράδειγμα εξόδου του βελτιστοποιημένου συστήματος συστάσεων για την ταίνία [\"Q Planes\"](https://en.wikipedia.org/wiki/Q_Planes) με την κλήση της συνάρτησης για κάποιο seed `content_recommender(529,3)`. Είναι φανερό ότι η κοινή θεματική των ταινιών είναι τα αεροπλάνα, οι πτήσεις, οι πιλότοι, ο πόλεμος."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI9FVjoEffMp"
      },
      "source": [
        "Για τη βελτιστοποίηση του συστήματος συστάσεων, απαιτείται ουσιαστικά η βελτιστοποίηση του συνόλου `corpus_tf_idf` που παράγεται από τον `TfidfVectorizer`. Ταυτοχρόνως, θέλουμε να μειώσουμε τον αριθμό χαρακτηριστικών που περιγράφουν τα δεδομένα μας για πιο γρήγορη εκπαίδευση πάνω σε αυτά. Για αυτό το σκοπό δοκιμάστηκαν διάφοροι συνδυασμοί παραμέτρων και μελετήθηκαν τα αποτελέσματα των ταινιών με id 42 και 120, τα οποία παρατηρήθηκαν από την αρχή να έχουν σχετικά καλές συστάσεις.<br/>\r\n",
        "Αρχικά, θέσαμε το `min_df` στο 2 και το `max_df` στο 0.5. Σκοπός είναι με το πρώτο να εξαλείψουμε λέξεις που εμφανίζονται ελάχιστες φορές και άρα δεν προσθέτουν ιδιαίτερα στη διακριτική μας ικανότητα, ενώ με το δεύτερο να εξαλείψουμε λέξεις που εμφανίζονται στα περισσότερα κείμενα και άρα δε βοηθούν στο διαχωρισμό μεταξύ τους. Εκκινούμε από αυτές τις τιμές και στη συνέχεια αναζητούμε ποιες βελτιστοποιούν τα αποτελέσματα.<br/>\r\n",
        "Κατά τη διαδικασία αυτή παρατηρήθηκε ότι για ένα μεγάλο εύρος του `max_df` η μείωσή του δεν επηρέαζε τα αποτελέσματα. Για αυτό το λόγο κατέληξε με αρκετά μικρότερη τιμή. Το `min_df` άλλαξε τελικά από απόλυτη τιμή σε σχετική.<br/>\r\n",
        "Πέραν αυτών, δοκιμάστηκε η χρήση των παραμέτρων `sublinear_tf`, `binary` και η αλλαγή της `norm`. Επίσης, ελέγχθηκε η χρήση μεγαλύτερου εύρους ngram_range για να συμπεριλάβουμε χαρακτηριστικά βασισμένα σε συνδυασμούς λέξεων. Ωστόσο, κανένα από αυτά δεν επηρέασε τελικά θετικά τα δεδομένα (μόνα τους ή σε συνδυαμό), οπότε δεν επιλέχτηκαν για τον βελτιστοποιημένο vectorizer.<br/>\r\n",
        "Ελέγχοντας μόνο τις παραπάνω παραμέτρους όμως τα αποτελέσματα δεν βελτιώνονταν αρκετά, ενώ παραμέναμε σε έναν αριθμό χαρακτηριστικών γύρω στα 12000-17000, κατώ από τον οποίο αρχίζαμε να χάνουμε επίδοση. Για αυτο το λόγο, τελικά χρησιμοποιήθηκε η παράμετρος `max_features` η οποία θέτει μέγιστο αριθμό των χαρακτηριστικών που περιγράφουν το corpus (επιλέγονται οι όροι με μεγαλύτερη συχνότητα). Μειώοντας αρκετά την τιμή αυτού, όχι μόνο λύθηκε το πρόβλημα της διαστατικότητας, αλλά επίσης, παρατηρήθηκε καλύτερη συμπεριφορά από το σύστημα συστάσεως. Ο λόγος για αυτό μπορεί να αποδοθεί στο ότι μπορούν πιο εύκολα να βγουν καλά συμπεράσματα όταν υπάρχει μικρότερος αριθμός χαρακτηριστικών να αναλογιστούν. Αρκεί, βεβαίως, αυτά να περιγράφουν τα δεδομένα αρκετά καλά και για αυτό τον λόγο η υπερβολική μείωση θα μείωνε την ακρίβεις αποτελεσμάτων. Έτσι, επιλέχτηκε ως βέλτιστος αριθμός το 2000."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOyDwcufjGoE"
      },
      "source": [
        "Ακολουθούν μερικά παραδείγματα στα οποία τα αποτελέσματα ήταν αρκετά καλά για τις τουλάχιστον 5 υψηλότερες συστάσεις. Δίνονται τα ID των ταινιών καθώς και η θεματική που τις ενώνει με τις αντίστοιχες προτεινόμενες:\n",
        "\n",
        "* 120: δολοφονίες, ίντριγκα, μυστήριο\n",
        "* 42: Ινδία\n",
        "* 113: Ασία, Ιαπωνία, Japan και σε μειωμένο βαθμό: ταινίες πολέμου, δράση\n",
        "* 943: Βιντεοκασέτες (tapes) και σε μειωμένο βαθμό: εγκληματικότητα, θρίλερ\n",
        "* 2178: Θάλασσα, καρχαρίες, γιοτ\n",
        "* 4101: Ηθοποιία, Show Business\n",
        "* 3260: Αλκοολισμός\n",
        "* 4821: Ινδία, Μπόλυγουντ και σε μικρότερο βαθμό: γαμός\n",
        "* 3729: Πόλεμος, στρατός, στρατιώτες\n",
        "* 3403: Πόλεμος, Ναζί, Δεύτερος Παγκόσμιος Πόλεμος"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4irg4K-IPSym"
      },
      "source": [
        "## Tip: persistence αντικειμένων με joblib.dump\n",
        "\n",
        "H βιβλιοθήκη [joblib](https://pypi.python.org/pypi/joblib) της Python δίνει κάποιες εξαιρετικά χρήσιμες ιδιότητες στην ανάπτυξη κώδικα: pipelining, παραλληλισμό, caching και variable persistence. Τις τρεις πρώτες ιδιότητες τις είδαμε στην πρώτη άσκηση. Στην παρούσα άσκηση θα μας φανεί χρήσιμη η τέταρτη, το persistence των αντικειμένων. Συγκεκριμένα μπορούμε με:\n",
        "\n",
        "```python\n",
        "joblib.dump(my_object, 'my_object.pkl') \n",
        "```\n",
        "\n",
        "να αποθηκεύσουμε οποιοδήποτε αντικείμενο-μεταβλητή (εδώ το `my_object`) απευθείας πάνω στο filesystem ως αρχείο, το οποίο στη συνέχεια μπορούμε να ανακαλέσουμε ως εξής:\n",
        "\n",
        "```python\n",
        "my_object = joblib.load('my_object.pkl')\n",
        "```\n",
        "\n",
        "Μπορούμε έτσι να ανακαλέσουμε μεταβλητές ακόμα και αφού κλείσουμε και ξανανοίξουμε το notebook, χωρίς να χρειαστεί να ακολουθήσουμε ξανά όλα τα βήματα ένα - ένα για την παραγωγή τους, κάτι ιδιαίτερα χρήσιμο αν αυτή η διαδικασία είναι χρονοβόρα.\n",
        "\n",
        "Ας αποθηκεύσουμε το `corpus_tf_idf` και στη συνέχεια ας το ανακαλέσουμε."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aESOPYQaPSyo",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34f11bf6-480c-49d7-cabe-b3dbf89a6ab0"
      },
      "source": [
        "import joblib\n",
        "joblib.dump(corpus_tf_idf, 'corpus_tf_idf0.003-0.025-2000.pkl') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['corpus_tf_idf0.003-0.025-2000.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_rAEj5ZPSy1"
      },
      "source": [
        "\n",
        "\n",
        "Μπορείτε με ένα απλό `!ls` να δείτε ότι το αρχείο `corpus_tf_idf.pkl` υπάρχει στο filesystem σας (== persistence):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhwXmTEIPSy3",
        "scrolled": true
      },
      "source": [
        "!ls -lh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cey5AbkO475S"
      },
      "source": [
        "και μπορούμε να τα διαβάσουμε με `joblib.load`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSJPTKY8PSyu"
      },
      "source": [
        "corpus_tf_idf = joblib.load('corpus_tf_idf0.003-0.025-300.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHOQtO83PSy9"
      },
      "source": [
        "# Εφαρμογή 2.  Τοπολογική και σημασιολογική απεικόνιση της ταινιών με χρήση SOM\n",
        "<img src=\"https://i.imgur.com/Z4FdurD.jpg\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB_clmizPSy-"
      },
      "source": [
        "## Δημιουργία dataset\n",
        "Στη δεύτερη εφαρμογή θα βασιστούμε στις τοπολογικές ιδιότητες των Self Organizing Maps (SOM) για να φτιάξουμε ενά χάρτη (grid) δύο διαστάσεων όπου θα απεικονίζονται όλες οι ταινίες της συλλογής της ομάδας με τρόπο χωρικά συνεκτικό ως προς το περιεχόμενο και κυρίως το είδος τους (ο παραπάνω χάρτης είναι ενδεικτικός, δεν αντιστοιχεί στο dataset μας). \n",
        "\n",
        "Η `build_final_set` αρχικά μετατρέπει την αραιή αναπαράσταση tf-idf της εξόδου της `TfidfVectorizer()` σε πυκνή (η [αραιή αναπαράσταση](https://en.wikipedia.org/wiki/Sparse_matrix) έχει τιμές μόνο για τα μη μηδενικά στοιχεία). \n",
        "\n",
        "Στη συνέχεια ενώνει την πυκνή `dense_tf_idf` αναπαράσταση και τις binarized κατηγορίες `catbins` των ταινιών ως επιπλέον στήλες (χαρακτηριστικά). Συνεπώς, κάθε ταινία αναπαρίσταται στο Vector Space Model από τα χαρακτηριστικά του TFIDF και τις κατηγορίες της.\n",
        "\n",
        "Τέλος, δέχεται ένα ορισμα για το πόσες ταινίες να επιστρέψει, με default τιμή όλες τις ταινίες (5000). Αυτό είναι χρήσιμο για να μπορείτε αν θέλετε να φτιάχνετε μικρότερα σύνολα δεδομένων ώστε να εκπαιδεύεται ταχύτερα το SOM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-FDDOkQPSzA"
      },
      "source": [
        "def build_final_set(doc_limit = 5000, tf_idf_only=False):\n",
        "    # convert sparse tf_idf to dense tf_idf representation\n",
        "    dense_tf_idf = corpus_tf_idf.toarray()[0:doc_limit,:]\n",
        "    if tf_idf_only:\n",
        "        # use only tf_idf\n",
        "        final_set = dense_tf_idf\n",
        "    else:\n",
        "        # append the binary categories features horizontaly to the (dense) tf_idf features\n",
        "        final_set = np.hstack((dense_tf_idf, catbins[0:doc_limit,:]))\n",
        "        # η somoclu θέλει δεδομένα σε float32\n",
        "    return np.array(final_set, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF1B62UbPSzF"
      },
      "source": [
        "final_set = build_final_set(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjvPPENS_dYL"
      },
      "source": [
        "Τυπώνουμε τις διαστάσεις του τελικού dataset μας. Χωρίς βελτιστοποίηση του TFIDF θα έχουμε περίπου 50.000 χαρακτηριστικά."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvEgNn-L-jEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00cc404a-ab57-4028-84e2-44ab88cfa925"
      },
      "source": [
        "final_set.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 2322)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om7PoyDVCqis"
      },
      "source": [
        "Με βάση την εμπειρία σας στην προετοιμασία των δεδομένων στην επιβλεπόμενη μάθηση, υπάρχει κάποιο βήμα προεπεξεργασίας που θα μπορούσε να εφαρμοστεί σε αυτό το dataset; "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J516AyRluqej"
      },
      "source": [
        "Για τη μείωση της διαστατικότητας μας είναι από την επιβλεπόμενη μάθηση γνωστό το βήμα προεπεξεργασίας PCA το οποίο συνδυάζει πολλά χαρακτηριστικά σε ένα και άρα έχει ως αποτέλεσμα τη μείωση των συνολικών χαρακτηριστικών, χωρίς όμως να θυσιάζει πληροφορία."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tikdip0PSzQ"
      },
      "source": [
        "## Εκπαίδευση χάρτη SOM\n",
        "\n",
        "Θα δουλέψουμε με τη βιβλιοθήκη SOM [\"Somoclu\"](http://somoclu.readthedocs.io/en/stable/index.html). Εισάγουμε τις somoclu και matplotlib και λέμε στη matplotlib να τυπώνει εντός του notebook (κι όχι σε pop up window)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX9rzxGSPSzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0727208d-931c-4269-e427-19a92a95f0cc"
      },
      "source": [
        "# install somoclu\n",
        "!pip install --upgrade somoclu\n",
        "# import sompoclu, matplotlib\n",
        "import somoclu\n",
        "import matplotlib\n",
        "# we will plot inside the notebook and not in separate window\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting somoclu\n",
            "  Downloading somoclu-1.7.5.1.tar.gz (6.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.1 MB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from somoclu) (1.19.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from somoclu) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from somoclu) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->somoclu) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->somoclu) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->somoclu) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->somoclu) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->somoclu) (1.15.0)\n",
            "Building wheels for collected packages: somoclu\n",
            "  Building wheel for somoclu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for somoclu: filename=somoclu-1.7.5.1-cp36-cp36m-linux_x86_64.whl size=421014 sha256=7bd8ea20520b2e9edd54ef18114f3ec4ebe31f9925fdb8cd6e7efd55de2ca195\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/4c/18/b5f05a068134b3d70d4060a06d2cd9080fa21084dc876f51e3\n",
            "Successfully built somoclu\n",
            "Installing collected packages: somoclu\n",
            "Successfully installed somoclu-1.7.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqBfn0ijPSzX"
      },
      "source": [
        "Καταρχάς διαβάστε το [function reference](http://somoclu.readthedocs.io/en/stable/reference.html) του somoclu. Θα δoυλέψουμε με χάρτη τύπου planar, παραλληλόγραμμου σχήματος νευρώνων με τυχαία αρχικοποίηση (όλα αυτά είναι default). Μπορείτε να δοκιμάσετε διάφορα μεγέθη χάρτη ωστόσο όσο ο αριθμός των νευρώνων μεγαλώνει, μεγαλώνει και ο χρόνος εκπαίδευσης. Για το training δεν χρειάζεται να ξεπεράσετε τα 100 epochs. Σε γενικές γραμμές μπορούμε να βασιστούμε στις default παραμέτρους μέχρι να έχουμε τη δυνατότητα να οπτικοποιήσουμε και να αναλύσουμε ποιοτικά τα αποτελέσματα. Ξεκινήστε με ένα χάρτη 10 x 10, 100 epochs training και ένα υποσύνολο των ταινιών (π.χ. 2000). Χρησιμοποιήστε την `time` για να έχετε μια εικόνα των χρόνων εκπαίδευσης. Ενδεικτικά, με σωστή κωδικοποίηση tf-idf, μικροί χάρτες για λίγα δεδομένα (1000-2000) παίρνουν γύρω στο ένα λεπτό ενώ μεγαλύτεροι χάρτες με όλα τα δεδομένα μπορούν να πάρουν 10-15 λεπτά ή και περισσότερο.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INxV0BAhCUOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16361ebe-1fac-4000-8c26-b00c9bd0489c"
      },
      "source": [
        "som = somoclu.Somoclu(10, 10)\n",
        "\n",
        "%time som.train(final_set, 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 11s, sys: 156 ms, total: 1min 11s\n",
            "Wall time: 36.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntd2GE9SaHiS"
      },
      "source": [
        "\n",
        "## Best matching units\n",
        "\n",
        "Μετά από κάθε εκπαίδευση αποθηκεύστε σε μια μεταβλητή τα best matching units (bmus) για κάθε ταινία. Τα bmus μας δείχνουν σε ποιο νευρώνα ανήκει η κάθε ταινία. Προσοχή: η σύμβαση των συντεταγμένων των νευρώνων είναι (στήλη, γραμμή) δηλαδή το ανάποδο από την Python. Με χρήση της [np.unique](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.unique.html) (μια πολύ χρήσιμη συνάρτηση στην άσκηση) αποθηκεύστε τα μοναδικά best matching units και τους δείκτες τους (indices) προς τις ταινίες. Σημειώστε ότι μπορεί να έχετε λιγότερα μοναδικά bmus από αριθμό νευρώνων γιατί μπορεί σε κάποιους νευρώνες να μην έχουν ανατεθεί ταινίες. Ως αριθμό νευρώνα θα θεωρήσουμε τον αριθμό γραμμής στον πίνακα μοναδικών bmus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HGcyuyjCXpC"
      },
      "source": [
        "act_map = som.get_surface_state()\n",
        "bmus = np.unique(som.get_bmus(act_map), return_index=True, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grzqcyHyaKdg"
      },
      "source": [
        "\n",
        "## Ομαδοποίηση (clustering)\n",
        "\n",
        "Τυπικά, η ομαδοποίηση σε ένα χάρτη SOM προκύπτει από το unified distance matrix (U-matrix): για κάθε κόμβο υπολογίζεται η μέση απόστασή του από τους γειτονικούς κόμβους. Εάν χρησιμοποιηθεί μπλε χρώμα στις περιοχές του χάρτη όπου η τιμή αυτή είναι χαμηλή (μικρή απόσταση) και κόκκινο εκεί που η τιμή είναι υψηλή (μεγάλη απόσταση), τότε μπορούμε να πούμε ότι οι μπλε περιοχές αποτελούν clusters και οι κόκκινες αποτελούν σύνορα μεταξύ clusters.\n",
        "\n",
        "To somoclu δίνει την επιπρόσθετη δυνατότητα να κάνουμε ομαδοποίηση των νευρώνων χρησιμοποιώντας οποιονδήποτε αλγόριθμο ομαδοποίησης του scikit-learn. Στην άσκηση θα χρησιμοποιήσουμε τον k-Means. Για τον αρχικό σας χάρτη δοκιμάστε ένα k=20 ή 25. Οι δύο προσεγγίσεις ομαδοποίησης είναι διαφορετικές, οπότε περιμένουμε τα αποτελέσματα να είναι κοντά αλλά όχι τα ίδια.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un3UwCtxCb92"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(25)\n",
        "\n",
        "som.cluster(kmeans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzNBNYD5Ci3L"
      },
      "source": [
        "som.view_umatrix(figsize=(15, 15), bestmatches=True, colorbar=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvIteca8ClYg"
      },
      "source": [
        "print(som.clusters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nupuqcuaMe3"
      },
      "source": [
        "\n",
        "## Αποθήκευση του SOM\n",
        "\n",
        "Επειδή η αρχικοποίηση του SOM γίνεται τυχαία και το clustering είναι και αυτό στοχαστική διαδικασία, οι θέσεις και οι ετικέτες των νευρώνων και των clusters θα είναι διαφορετικές κάθε φορά που τρέχετε τον χάρτη, ακόμα και με τις ίδιες παραμέτρους. Για να αποθηκεύσετε ένα συγκεκριμένο som και clustering χρησιμοποιήστε και πάλι την `joblib`. Μετά την ανάκληση ενός SOM θυμηθείτε να ακολουθήσετε τη διαδικασία για τα bmus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4a74j3fConG"
      },
      "source": [
        "joblib.dump(som, 'som-1000-20x20.pkl')\n",
        "joblib.dump(bmus, 'bmus-1000-20x20.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejX0Qs18aRHU"
      },
      "source": [
        "\n",
        "## Οπτικοποίηση U-matrix, clustering και μέγεθος clusters\n",
        "\n",
        "Για την εκτύπωση του U-matrix χρησιμοποιήστε τη `view_umatrix` με ορίσματα `bestmatches=True` και `figsize=(15, 15)` ή `figsize=(20, 20)`. Τα διαφορετικά χρώματα που εμφανίζονται στους κόμβους αντιπροσωπεύουν τα διαφορετικά clusters που προκύπτουν από τον k-Means. Μπορείτε να εμφανίσετε τη λεζάντα του U-matrix με το όρισμα `colorbar`. Μην τυπώνετε τις ετικέτες (labels) των δειγμάτων, είναι πολύ μεγάλος ο αριθμός τους.\n",
        "\n",
        "Για μια δεύτερη πιο ξεκάθαρη οπτικοποίηση του clustering τυπώστε απευθείας τη μεταβλητή `clusters`.\n",
        "\n",
        "Τέλος, χρησιμοποιώντας πάλι την `np.unique` (με διαφορετικό όρισμα) και την `np.argsort` (υπάρχουν και άλλοι τρόποι υλοποίησης) εκτυπώστε τις ετικέτες των clusters (αριθμοί από 0 έως k-1) και τον αριθμό των νευρώνων σε κάθε cluster, με φθίνουσα ή αύξουσα σειρά ως προς τον αριθμό των νευρώνων. Ουσιαστικά είναι ένα εργαλείο για να βρίσκετε εύκολα τα μεγάλα και μικρά clusters. \n",
        "\n",
        "Ακολουθεί ένα μη βελτιστοποιημένο παράδειγμα για τις τρεις προηγούμενες εξόδους:\n",
        "\n",
        "<img src=\"https://image.ibb.co/i0tsfR/umatrix_s.jpg\" width=\"35%\">\n",
        "<img src=\"https://image.ibb.co/nLgHEm/clusters.png\" width=\"35%\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMO_KcQYaTv-"
      },
      "source": [
        "\n",
        "## Σημασιολογική ερμηνεία των clusters\n",
        "\n",
        "Προκειμένου να μελετήσουμε τις τοπολογικές ιδιότητες του SOM και το αν έχουν ενσωματώσει σημασιολογική πληροφορία για τις ταινίες διαμέσου της διανυσματικής αναπαράστασης με το tf-idf και των κατηγοριών, χρειαζόμαστε ένα κριτήριο ποιοτικής επισκόπησης των clusters. Θα υλοποιήσουμε το εξής κριτήριο: Λαμβάνουμε όρισμα έναν αριθμό (ετικέτα) cluster. Για το cluster αυτό βρίσκουμε όλους τους νευρώνες που του έχουν ανατεθεί από τον k-Means. Για όλους τους νευρώνες αυτούς βρίσκουμε όλες τις ταινίες που τους έχουν ανατεθεί (για τις οποίες αποτελούν bmus). Για όλες αυτές τις ταινίες τυπώνουμε ταξινομημένη τη συνολική στατιστική όλων των ειδών (κατηγοριών) και τις συχνότητές τους. Αν το cluster διαθέτει καλή συνοχή και εξειδίκευση, θα πρέπει κάποιες κατηγορίες να έχουν σαφώς μεγαλύτερη συχνότητα από τις υπόλοιπες. Θα μπορούμε τότε να αναθέσουμε αυτήν/ές την/τις κατηγορία/ες ως ετικέτες κινηματογραφικού είδους στο cluster.\n",
        "\n",
        "Μπορείτε να υλοποιήσετε τη συνάρτηση αυτή όπως θέλετε. Μια πιθανή διαδικασία θα μπορούσε να είναι η ακόλουθη:\n",
        "\n",
        "1. Ορίζουμε συνάρτηση `print_categories_stats` που δέχεται ως είσοδο λίστα με ids ταινιών. Δημιουργούμε μια κενή λίστα συνολικών κατηγοριών. Στη συνέχεια, για κάθε ταινία επεξεργαζόμαστε το string `categories` ως εξής: δημιουργούμε μια λίστα διαχωρίζοντας το string κατάλληλα με την `split` και αφαιρούμε τα whitespaces μεταξύ ετικετών με την `strip`. Προσθέτουμε τη λίστα αυτή στη συνολική λίστα κατηγοριών με την `extend`. Τέλος χρησιμοποιούμε πάλι την `np.unique` για να μετρήσουμε συχνότητα μοναδικών ετικετών κατηγοριών και ταξινομούμε με την `np.argsort`. Τυπώνουμε τις κατηγορίες και τις συχνότητες εμφάνισης ταξινομημένα. Χρήσιμες μπορεί να σας φανούν και οι `np.ravel`, `np.nditer`, `np.array2string` και `zip`.\n",
        "\n",
        "2. Ορίζουμε τη βασική μας συνάρτηση `print_cluster_neurons_movies_report` που δέχεται ως όρισμα τον αριθμό ενός cluster. Με τη χρήση της `np.where` μπορούμε να βρούμε τις συντεταγμένες των bmus που αντιστοιχούν στο cluster και με την `column_stack` να φτιάξουμε έναν πίνακα bmus για το cluster. Προσοχή στη σειρά (στήλη - σειρά) στον πίνακα bmus. Για κάθε bmu αυτού του πίνακα ελέγχουμε αν υπάρχει στον πίνακα μοναδικών bmus που έχουμε υπολογίσει στην αρχή συνολικά και αν ναι προσθέτουμε το αντίστοιχο index του νευρώνα σε μια λίστα. Χρήσιμες μπορεί να είναι και οι `np.rollaxis`, `np.append`, `np.asscalar`. Επίσης πιθανώς να πρέπει να υλοποιήσετε ένα κριτήριο ομοιότητας μεταξύ ενός bmu και ενός μοναδικού bmu από τον αρχικό πίνακα bmus.\n",
        "\n",
        "3. Υλοποιούμε μια βοηθητική συνάρτηση `neuron_movies_report`. Λαμβάνει ένα σύνολο νευρώνων από την `print_cluster_neurons_movies_report` και μέσω της `indices` φτιάχνει μια λίστα με το σύνολο ταινιών που ανήκουν σε αυτούς τους νευρώνες. Στο τέλος καλεί με αυτή τη λίστα την `print_categories_stats` που τυπώνει τις στατιστικές των κατηγοριών.\n",
        "\n",
        "Μπορείτε βέβαια να προσθέσετε οποιαδήποτε επιπλέον έξοδο σας βοηθάει. Μια χρήσιμη έξοδος είναι πόσοι νευρώνες ανήκουν στο cluster και σε πόσους και ποιους από αυτούς έχουν ανατεθεί ταινίες.\n",
        "\n",
        "Θα επιτελούμε τη σημασιολογική ερμηνεία του χάρτη καλώντας την `print_cluster_neurons_movies_report` με τον αριθμός ενός cluster που μας ενδιαφέρει. \n",
        "\n",
        "Παράδειγμα εξόδου για ένα cluster (μη βελτιστοποιημένος χάρτης, ωστόσο βλέπετε ότι οι μεγάλες κατηγορίες έχουν σημασιολογική  συνάφεια):\n",
        "\n",
        "```\n",
        "Overall Cluster Genres stats:  \n",
        "[('\"Horror\"', 86), ('\"Science Fiction\"', 24), ('\"B-movie\"', 16), ('\"Monster movie\"', 10), ('\"Creature Film\"', 10), ('\"Indie\"', 9), ('\"Zombie Film\"', 9), ('\"Slasher\"', 8), ('\"World cinema\"', 8), ('\"Sci-Fi Horror\"', 7), ('\"Natural horror films\"', 6), ('\"Supernatural\"', 6), ('\"Thriller\"', 6), ('\"Cult\"', 5), ('\"Black-and-white\"', 5), ('\"Japanese Movies\"', 4), ('\"Short Film\"', 3), ('\"Drama\"', 3), ('\"Psychological thriller\"', 3), ('\"Crime Fiction\"', 3), ('\"Monster\"', 3), ('\"Comedy\"', 2), ('\"Western\"', 2), ('\"Horror Comedy\"', 2), ('\"Archaeology\"', 2), ('\"Alien Film\"', 2), ('\"Teen\"', 2), ('\"Mystery\"', 2), ('\"Adventure\"', 2), ('\"Comedy film\"', 2), ('\"Combat Films\"', 1), ('\"Chinese Movies\"', 1), ('\"Action/Adventure\"', 1), ('\"Gothic Film\"', 1), ('\"Costume drama\"', 1), ('\"Disaster\"', 1), ('\"Docudrama\"', 1), ('\"Film adaptation\"', 1), ('\"Film noir\"', 1), ('\"Parody\"', 1), ('\"Period piece\"', 1), ('\"Action\"', 1)]```\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq4QrImhaa7E"
      },
      "source": [
        "\n",
        "## Tips για το SOM και το clustering\n",
        "\n",
        "- Για την ομαδοποίηση ένα U-matrix καλό είναι να εμφανίζει και μπλε-πράσινες περιοχές (clusters) και κόκκινες περιοχές (ορίων). Παρατηρήστε ποια σχέση υπάρχει μεταξύ αριθμού ταινιών στο final set, μεγέθους grid και ποιότητας U-matrix.\n",
        "- Για το k του k-Means προσπαθήστε να προσεγγίζει σχετικά τα clusters του U-matrix (όπως είπαμε είναι διαφορετικοί μέθοδοι clustering). Μικρός αριθμός k δεν θα σέβεται τα όρια. Μεγάλος αριθμός θα δημιουργεί υπο-clusters εντός των clusters που φαίνονται στο U-matrix. Το τελευταίο δεν είναι απαραίτητα κακό, αλλά μεγαλώνει τον αριθμό clusters που πρέπει να αναλυθούν σημασιολογικά.\n",
        "- Σε μικρούς χάρτες και με μικρά final sets δοκιμάστε διαφορετικές παραμέτρους για την εκπαίδευση του SOM. Σημειώστε τυχόν παραμέτρους που επηρεάζουν την ποιότητα του clustering για το dataset σας ώστε να τις εφαρμόσετε στους μεγάλους χάρτες.\n",
        "- Κάποια τοπολογικά χαρακτηριστικά εμφανίζονται ήδη σε μικρούς χάρτες. Κάποια άλλα χρειάζονται μεγαλύτερους χάρτες. Δοκιμάστε μεγέθη 20x20, 25x25 ή και 30x30 και αντίστοιχη προσαρμογή των k. Όσο μεγαλώνουν οι χάρτες, μεγαλώνει η ανάλυση του χάρτη αλλά μεγαλώνει και ο αριθμός clusters που πρέπει να αναλυθούν.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4IUl8O8ayVf"
      },
      "source": [
        "\n",
        "\n",
        "## Ανάλυση τοπολογικών ιδιοτήτων χάρτη SOM\n",
        "\n",
        "Μετά το πέρας της εκπαίδευσης και του clustering θα έχετε ένα χάρτη με τοπολογικές ιδιότητες ως προς τα είδη των ταίνιών της συλλογής σας, κάτι αντίστοιχο με την εικόνα στην αρχή της Εφαρμογής 2 αυτού του notebook (η συγκεκριμένη εικόνα είναι μόνο για εικονογράφιση, δεν έχει καμία σχέση με τη συλλογή δεδομένων και τις κατηγορίες μας).\n",
        "\n",
        "Για τον τελικό χάρτη SOM που θα παράξετε για τη συλλογή σας, αναλύστε σε markdown με συγκεκριμένη αναφορά σε αριθμούς clusters και τη σημασιολογική ερμηνεία τους τις εξής τρεις τοπολογικές ιδιότητες του SOM: \n",
        "\n",
        "1. Δεδομένα που έχουν μεγαλύτερη πυκνότητα πιθανότητας στο χώρο εισόδου τείνουν να απεικονίζονται με περισσότερους νευρώνες στο χώρο μειωμένης διαστατικότητας. Δώστε παραδείγματα από συχνές και λιγότερο συχνές κατηγορίες ταινιών. Χρησιμοποιήστε τις στατιστικές των κατηγοριών στη συλλογή σας και τον αριθμό κόμβων που χαρακτηρίζουν.\n",
        "2. Μακρινά πρότυπα εισόδου τείνουν να απεικονίζονται απομακρυσμένα στο χάρτη. Υπάρχουν χαρακτηριστικές κατηγορίες ταινιών που ήδη από μικρούς χάρτες τείνουν να τοποθετούνται σε διαφορετικά ή απομονωμένα σημεία του χάρτη.\n",
        "3. Κοντινά πρότυπα εισόδου τείνουν να απεικονίζονται κοντά στο χάρτη. Σε μεγάλους χάρτες εντοπίστε είδη ταινιών και κοντινά τους υποείδη.\n",
        "\n",
        "Προφανώς τοποθέτηση σε 2 διαστάσεις που να σέβεται μια απόλυτη τοπολογία δεν είναι εφικτή, αφενός γιατί δεν υπάρχει κάποια απόλυτη εξ ορισμού για τα κινηματογραφικά είδη ακόμα και σε πολλές διαστάσεις, αφετέρου γιατί πραγματοποιούμε μείωση διαστατικότητας.\n",
        "\n",
        "Εντοπίστε μεγάλα clusters και μικρά clusters που δεν έχουν σαφή χαρακτηριστικά. Εντοπίστε clusters συγκεκριμένων ειδών που μοιάζουν να μην έχουν τοπολογική συνάφεια με γύρω περιοχές. Προτείνετε πιθανές ερμηνείες.\n",
        "\n",
        "\n",
        "\n",
        "Τέλος, εντοπίστε clusters που έχουν κατά την άποψή σας ιδιαίτερο ενδιαφέρον στη συλλογή της ομάδας σας (data exploration / discovery value) και σχολιάστε.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYjxGR5DawIy"
      },
      "source": [
        "\n",
        "# Τελική παράδοση άσκησης\n",
        "\n",
        "- Θα παραδώσετε στο eclass το παρόν notebook επεξεργασμένο ή ένα νέο με τις απαντήσεις σας για τα ζητούμενα και των δύο εφαρμογών. \n",
        "- Θυμηθείτε ότι η ανάλυση του χάρτη στο markdown με αναφορά σε αριθμούς clusters πρέπει να αναφέρεται στον τελικό χάρτη με τα κελιά ορατά που θα παραδώσετε αλλιώς ο χάρτης που θα προκύψει θα είναι διαφορετικός και τα labels των clusters δεν θα αντιστοιχούν στην ανάλυσή σας. \n",
        "- Μην ξεχάσετε στην αρχή ένα κελί markdown με **τα στοιχεία της ομάδας σας**.\n",
        "- Στο **zip** που θα παραδώσετε πρέπει να βρίσκονται **2 αρχεία (το .ipynb και το .py του notebook σας)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHhCkvxjnitd"
      },
      "source": [
        "<table>\n",
        "  <tr><td align=\"center\">\n",
        "    <font size=\"4\">Παρακαλούμε διατρέξτε βήμα-βήμα το notebook για να μην ξεχάσετε παραδοτέα!</font>\n",
        "</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    }
  ]
}